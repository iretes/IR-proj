{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Scale Experiments\n",
    "\n",
    "Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, SpectralBiclustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from k_means_constrained import KMeansConstrained\n",
    "from search_approaches import PQ, IVF, ExactSearch\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "DATASET = \"siftsmall\" # \"glove\", \"gist\"\n",
    "DIR = \"siftsmall\" # \"glove\", \"gist\"\n",
    "SEARCH_TRAIN_SUBSET = False # True if vectors to search in are a subset of the training set\n",
    "tab20 = matplotlib.colormaps[\"tab20\"]\n",
    "tab20c = matplotlib.colormaps[\"tab20c\"]\n",
    "tab10 = matplotlib.colormaps[\"tab10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data.\n",
    "\n",
    "NOTE: GloVe embeddings are normalized to have unit norm, ensuring that the squared Euclidean distance is proportional to cosine similarity (commonly used for comparing word embeddings), differing only by a constant factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, search_data, queries, gt = load_data(\n",
    "    dataset_name=DATASET,\n",
    "    dataset_dir=DIR,\n",
    "    search_train_subset=SEARCH_TRAIN_SUBSET,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "sample_query = queries[0]\n",
    "sample_query_snorm = np.sum(np.square(sample_query))\n",
    "print(\"Base vectors (to search in) shape: \", search_data.shape)\n",
    "print(f\"Base vectors (to search in) range: [{search_data.min()}, {search_data.max()}]\")\n",
    "print(\"Query vectors shape: \", queries.shape)\n",
    "if DATASET == \"siftsmall\":\n",
    "    print(\"Ground truth shape: \", gt.shape)\n",
    "print(\"Learn vectors shape: \", tr_data.shape)\n",
    "print(\"Query example:\\n\", queries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([tr_data, search_data, queries], axis=0)\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "tr_data_pca = data_pca[:len(tr_data)]\n",
    "search_data_pca = data_pca[len(tr_data) : len(tr_data)+len(search_data)]\n",
    "queries_pca = data_pca[len(tr_data)+len(search_data) : ]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "axs[0].scatter(x=tr_data_pca[:, 0], y=tr_data_pca[:, 1], color='blue', label='Training data', edgecolors='white')\n",
    "axs[1].scatter(x=search_data_pca[:, 0], y=search_data_pca[:, 1], color='red', label='Search data', edgecolors='white')\n",
    "axs[2].scatter(x=queries_pca[:, 0], y=queries_pca[:, 1], color='green', label='Queries', edgecolors='white')\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].set_xlabel('PCA Dim. 1')\n",
    "    axs[i].set_ylabel('PCA Dim. 2')\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.title(\"Data Visualization with PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"siftsmall\": # glove and gift data have too many features\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    plt.boxplot(tr_data)\n",
    "    plt.title(\"Training data\")\n",
    "    plt.xticks([])\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    plt.boxplot(search_data)\n",
    "    plt.title(\"Search data\")\n",
    "    plt.xticks([])\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    plt.boxplot(queries)\n",
    "    plt.title(\"Queries\")\n",
    "    plt.xticks([])\n",
    "    plt.xlabel(\"Features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of the results from the article\n",
    "\n",
    "### Product Quantization\n",
    "\n",
    "Training the PQ quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"glove\":\n",
    "    M = 20\n",
    "elif DATASET == \"gist\":\n",
    "    M = 60\n",
    "else:\n",
    "    M = 8\n",
    "K = 256\n",
    "pq = PQ(M=M, K=K, seed=RANDOM_SEED)\n",
    "\n",
    "print(f\"Training the PQ quantizer with m={M}, k*={K}\"\n",
    "    f\" (chunk size = {tr_data.shape[1] // M})...\")\n",
    "\n",
    "start_training = time.time()\n",
    "pq.train(tr_data, add=False, verbose=True)\n",
    "training_time = time.time() - start_training\n",
    "start_adding = time.time()\n",
    "pq.add(search_data)\n",
    "adding_time = time.time() - start_adding\n",
    "\n",
    "compressed_tr = pq.compress(tr_data)\n",
    "decompressed_tr = pq.decompress(compressed_tr) \n",
    "error_tr = NMSE(tr_data, decompressed_tr)\n",
    "\n",
    "decompressed_search = pq.decompress(pq.pqcode)\n",
    "error_search = NMSE(search_data, decompressed_search)\n",
    "\n",
    "results = [{\n",
    "    \"Training time [s]\": training_time,\n",
    "    \"Adding time [s]\": adding_time,\n",
    "    \"Compressed data shape\": f\"{pq.pqcode.shape}\",\n",
    "    \"Compressed data size [bytes]\": pq.pqcode.nbytes,\n",
    "    \"Original data size [bytes]\": search_data.nbytes,\n",
    "    \"Compression factor\": search_data.nbytes / pq.pqcode.nbytes,\n",
    "    \"NMSE on training data\": error_tr,\n",
    "    \"NMSE on search data\": error_search\n",
    "}]\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(f\"Training the PQ quantizer with m={M}, k*={K}\"\n",
    "    f\" (chunk size = {tr_data.shape[1] // M}) and average distortion\"\n",
    "    \" computation...\")\n",
    "\n",
    "start_training_d = time.time()\n",
    "pq.train(tr_data, add=False, verbose=True)\n",
    "training_time_d = time.time() - start_training_d\n",
    "start_adding_d = time.time()\n",
    "pq.add(search_data, compute_distortions=True)\n",
    "adding_time_d = time.time() - start_adding_d\n",
    "\n",
    "results.append({\n",
    "    \"Training time [s]\": training_time_d,\n",
    "    \"Adding time [s]\": adding_time_d,\n",
    "    \"Compressed data shape\": f\"{pq.pqcode.shape} + {pq.avg_dist.shape}\",\n",
    "    \"Compressed data size [bytes]\": pq.pqcode.nbytes + pq.avg_dist.nbytes,\n",
    "    \"Original data size [bytes]\": search_data.nbytes,\n",
    "    \"Compression factor\": search_data.nbytes / (pq.pqcode.nbytes + pq.avg_dist.nbytes),\n",
    "    \"NMSE on training data\": error_tr,\n",
    "    \"NMSE on search data\": error_search\n",
    "})\n",
    "\n",
    "pq_results_df = pd.DataFrame(\n",
    "    results,\n",
    "    index=[\"PQ without average distortion\", \"PQ with average distortion\"]\n",
    "    )\n",
    "pq_results_df.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the codes distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"glove\":\n",
    "    nrows = 5\n",
    "elif DATASET == \"gist\":\n",
    "    nrows = 15\n",
    "else:\n",
    "    nrows = 2\n",
    "fig, axs = plt.subplots(nrows, 4, figsize=(20, 3*nrows), sharey=True)\n",
    "id = 0\n",
    "for m in range(pq.M):\n",
    "    hist, bins = np.histogram(pq.pqcode[:, m], bins=range(pq.K+1))\n",
    "    hist = sorted(hist, reverse=True)\n",
    "    axs[int(id/4)][id%4].bar(bins[:-1], hist, width=1)\n",
    "    axs[int(id/4)][id%4].set_title(f\"Subspace {m+1}\")\n",
    "    axs[int(id/4)][id%4].set_xlabel(\"Codes sorted by frequency\")\n",
    "    axs[int(id/4)][id%4].set_ylabel(\"Frequency\")\n",
    "    axs[int(id/4)][id%4].grid()\n",
    "    axs[int(id/4)][id%4].set_axisbelow(True)\n",
    "    id += 1\n",
    "plt.suptitle(\"Codes distribution in each subspace\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the exact distances for a sample query and checking if they are equal to the given ground truth distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ExactSearch(search_data)\n",
    "exact_dists, exact_ranking = es.search(sample_query)\n",
    "if DATASET == \"siftsmall\" and not SEARCH_TRAIN_SUBSET:\n",
    "    print(np.all(gt[0][:100]==exact_ranking[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to plot exact vs approximate distances from a sample query for both symmetric and asymmetric approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_exact_vs_estimated(axs, exact_dists, sym_pq_dists, asym_pq_dists, query):\n",
    "    query_snorm = np.sum(np.square(query))\n",
    "    exact_dists = exact_dists / query_snorm\n",
    "    sym_pq_dists = sym_pq_dists / query_snorm\n",
    "    asym_pq_dists = asym_pq_dists / query_snorm\n",
    "\n",
    "    xy_sym = np.vstack([exact_dists, sym_pq_dists])\n",
    "    kernel_sym = gaussian_kde(xy_sym)(xy_sym)\n",
    "    axs.scatter(\n",
    "        exact_dists,\n",
    "        sym_pq_dists,\n",
    "        c=kernel_sym,\n",
    "        cmap=\"Reds\", edgecolor=\"black\", label=\"Symmetric\", zorder=2, marker=\"o\")\n",
    "    \n",
    "    xy_asym = np.vstack([exact_dists, asym_pq_dists])\n",
    "    kernel_asym = gaussian_kde(xy_asym)(xy_asym)\n",
    "    axs.scatter(\n",
    "        exact_dists,\n",
    "        asym_pq_dists,\n",
    "        c=kernel_asym,\n",
    "        cmap=\"Blues\", edgecolor=\"black\", label=\"Asymmetric\", zorder=2, marker=\"s\")\n",
    "\n",
    "    axs.set_xlabel(\"True distances (normalized)\")\n",
    "    axs.set_ylabel(\"Estimated distances (normalized)\")\n",
    "    \n",
    "    xpoints = axs.get_xlim()\n",
    "    axs.plot(xpoints, xpoints, linestyle=\"--\", color=\"black\", zorder=1)\n",
    "    \n",
    "    axs.grid(zorder=0)\n",
    "    axs.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the distances with PQ for a sample query and plotting their exact vs approximate values for a random subset of 200 database vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asym_pq_dists, asym_ranking = pq.search(sample_query, subset=None, asym=True, correct=False)\n",
    "asym_pq_corr_dists, asym_corr_ranking = pq.search(sample_query, subset=None, asym=True, correct=True)\n",
    "sym_pq_dists, sym_ranking = pq.search(sample_query, subset=None, asym=False, correct=False)\n",
    "sym_pq_corr_dists, sym_corr_ranking = pq.search(sample_query, subset=None, asym=False, correct=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sample_size = 1000\n",
    "plot_exact_vs_estimated(axs[0], exact_dists[:sample_size], sym_pq_dists[:sample_size], asym_pq_dists[:sample_size], sample_query)\n",
    "axs[0].set_title(\"Without correction\")\n",
    "plot_exact_vs_estimated(axs[1], exact_dists[:sample_size], sym_pq_corr_dists[:sample_size], asym_pq_corr_dists[:sample_size], sample_query)\n",
    "axs[1].set_title(\"With correction\");\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the estimated distances tends to underestimate the average distance between descriptors. The symmetric version is more affected by estimation bias compared to the asymmetric version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the differences between approximate and exact distances for all queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "asym_diffs = np.zeros((search_data.shape[0] * queries.shape[0]))\n",
    "asym_diffs_corr = np.zeros((search_data.shape[0] * queries.shape[0]))\n",
    "sym_diffs = np.zeros((search_data.shape[0] * queries.shape[0]))\n",
    "sym_diffs_corr = np.zeros((search_data.shape[0] * queries.shape[0]))\n",
    "\n",
    "exact_ranks = np.empty((queries.shape[0], search_data.shape[0]))\n",
    "mean_es_time = 0\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    asym_d_est, asym_rank = pq.search(query, subset=None, asym=True, correct=False)\n",
    "    asym_d_est_corr, asym_corr_rank = pq.search(query, subset=None, asym=True, correct=True)\n",
    "    sym_d_est, sym_rank = pq.search(query, subset=None, asym=False, correct=False)\n",
    "    sym_d_est_corr, sym_corr_rank = pq.search(query, subset=None, asym=False, correct=True)\n",
    "\n",
    "    start_search_time = time.time()\n",
    "    d, rank = es.search(query)\n",
    "    mean_es_time += time.time() - start_search_time\n",
    "    exact_ranks[i] = rank\n",
    "\n",
    "    query_snorm = np.sum(np.square(query))\n",
    "    asym_diffs[i*search_data.shape[0] : (i+1)*search_data.shape[0]] = (asym_d_est-d) / query_snorm\n",
    "    asym_diffs_corr[i*search_data.shape[0] : (i+1)*search_data.shape[0]] = (asym_d_est_corr-d) / query_snorm\n",
    "    sym_diffs[i*search_data.shape[0] : (i+1)*search_data.shape[0]] = (sym_d_est-d) / query_snorm\n",
    "    sym_diffs_corr[i*search_data.shape[0] : (i+1)*search_data.shape[0]] = (sym_d_est_corr-d) / query_snorm\n",
    "\n",
    "mean_es_time /= queries.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution of the computed differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(asym_diffs, label=\"ADC\", color=tab20(0))\n",
    "sns.kdeplot(asym_diffs_corr, label=\"Corrected ADC\", color=tab20(1), linestyle='dashed')\n",
    "sns.kdeplot(sym_diffs, label=\"SDC\", color=tab20(6))\n",
    "sns.kdeplot(sym_diffs_corr, label=\"Corrected SDC\", color=tab20(7), linestyle='dashed')\n",
    "plt.xlabel(\"Estimated distance - Exact distance (normalized)\")\n",
    "plt.ylabel(\"Empirical probability distribution function\")\n",
    "plt.legend();\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 30\n",
    "\n",
    "hist_asym, bin_edges_asym = np.histogram(asym_diffs, bins=bins, density=True)\n",
    "plt.step(bin_edges_asym[:-1], hist_asym, where='post', label=\"ADC\", color=tab20(0))\n",
    "\n",
    "hist_asym_corr, bin_edges_asym_corr = np.histogram(asym_diffs_corr, bins=bins, density=True)\n",
    "plt.step(bin_edges_asym_corr[:-1], hist_asym_corr, where='post', label=\"Corrected ADC\", color=tab20(1), linestyle='dashed')\n",
    "\n",
    "hist_sym, bin_edges_sym = np.histogram(sym_diffs, bins=bins, density=True)\n",
    "plt.step(bin_edges_sym[:-1], hist_sym, where='post', label=\"SDC\", color=tab20(6))\n",
    "\n",
    "hist_sym_corr, bin_edges_sym_corr = np.histogram(sym_diffs_corr, bins=bins, density=True)\n",
    "plt.step(bin_edges_sym_corr[:-1], hist_sym_corr, where='post', label=\"Corrected SDC\", color=tab20(7), linestyle='dashed')\n",
    "\n",
    "plt.xlabel(\"Estimated distance - Exact distance (normalized)\")\n",
    "plt.ylabel(\"Empirical probability distribution function\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias of the distance estimation is significantly reduced in the corrected version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the variance of the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asym_diff_var = np.var(asym_diffs)\n",
    "asym_corr_diff_var = np.var(asym_diffs_corr)\n",
    "sym_diff_var = np.var(sym_diffs)\n",
    "sym_corr_diff_var = np.var(sym_diffs_corr)\n",
    "pd.DataFrame({\n",
    "    \"ADC\": [asym_diff_var],\n",
    "    \"Corrected ADC\": [asym_corr_diff_var],\n",
    "    \"SDC\": [sym_diff_var],\n",
    "    \"Corrected SDC\": [sym_corr_diff_var],\n",
    "    }, index=[\"Variance of error\"]).T.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the \"siftsmall\" dataset, correcting the bias leads to a higher variance of the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to compute the average recall at various values of R for a set of queries. Both the \"nearest recall at R\" and \"recall at R\" are computed: the former measures the proportion of query vectors for which the nearest neighbor is ranked in the first R positions, while the latter measures the average recall at R. In the original article, only \"nearest recall at R is analyzed, with the authors noting that the conclusions are similar for recall at R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(index, R, queries, exact_ranks, w=4, correct=True, sym=True):\n",
    "    \"\"\"\n",
    "    Compute average recall at all values in R for the given index and queries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    index : PQ or IVF\n",
    "        The index to use for the search.\n",
    "    \n",
    "    R : list\n",
    "        The list of values for which to compute the recall at.\n",
    "\n",
    "    queries : np.ndarray\n",
    "        The queries to use for the search.\n",
    "\n",
    "    exact_ranks : np.ndarray\n",
    "        The exact ranking of the search data for each query.\n",
    "\n",
    "    w : int\n",
    "        The number of centroids to visit in the IVF index.\n",
    "\n",
    "    correct : bool\n",
    "        Whether to compute also the corrected distances.\n",
    "\n",
    "    sym : bool\n",
    "        Whether to compute also the symmetric distances.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    results : dict\n",
    "        A dictionary containing the mean recall at each value of R for the\n",
    "        asymmetric and symmetric distances, with and without correction, as\n",
    "        well as the attribute `inertia` of the index.\n",
    "        Recall is computed both as the number of relevant items in the top R,\n",
    "        and as the presence of the nearest item in the top R (nearest recall).\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    asym_recall = np.full((len(queries), len(R)), np.nan)\n",
    "    nearest_asym_recall = np.full((len(queries), len(R)), np.nan)\n",
    "    asym_corr_recall = np.full((len(queries), len(R)), np.nan)  \n",
    "    nearest_asym_corr_recall = np.full((len(queries), len(R)), np.nan)\n",
    "    sym_recall = np.full((len(queries), len(R)), np.nan)\n",
    "    nearest_sym_recall = np.full((len(queries), len(R)), np.nan)\n",
    "    sym_corr_recall = np.full((len(queries), len(R)), np.nan)\n",
    "    nearest_sym_corr_recall = np.full((len(queries), len(R)), np.nan)\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        if type(index) == PQ:\n",
    "            _, asym_rank = index.search(query, subset=None, asym=True, correct=False)\n",
    "            if correct:\n",
    "                _, asym_corr_rank = index.search(query, subset=None, asym=True, correct=True)\n",
    "            if sym:\n",
    "                _, sym_rank = index.search(query, subset=None, asym=False, correct=False)\n",
    "                if correct:\n",
    "                    _, sym_corr_rank = index.search(query, subset=None, asym=False, correct=True)\n",
    "        else:\n",
    "            _, asym_rank = index.search(query, w=w, asym=True, correct=False)\n",
    "            if correct:\n",
    "                _, asym_corr_rank = index.search(query, w=w, asym=True, correct=True)\n",
    "            if sym:\n",
    "                _, sym_rank = index.search(query, w=w, asym=False, correct=False)\n",
    "                if correct:\n",
    "                    _, sym_corr_rank = index.search(query, w=w, asym=False, correct=True)\n",
    "\n",
    "        for j, r in enumerate(R):\n",
    "            if r <= len(asym_rank):\n",
    "                asym_recall[i][j] = recall_at_r(asym_rank, exact_ranks[i], r)\n",
    "                nearest_asym_recall[i][j] = exact_ranks[i][0] in asym_rank[:r]\n",
    "            if correct and r <= len(asym_corr_rank):\n",
    "                asym_corr_recall[i][j] = recall_at_r(asym_corr_rank, exact_ranks[i], r)\n",
    "                nearest_asym_corr_recall[i][j] = exact_ranks[i][0] in asym_corr_rank[:r]\n",
    "            if sym:\n",
    "                if r <= len(sym_rank):\n",
    "                    sym_recall[i][j] = recall_at_r(sym_rank, exact_ranks[i], r)\n",
    "                    nearest_sym_recall[i][j] = exact_ranks[i][0] in sym_rank[:r]\n",
    "                if correct and r <= len(sym_corr_rank):\n",
    "                    sym_corr_recall[i][j] = recall_at_r(sym_corr_rank, exact_ranks[i], r)\n",
    "                    nearest_sym_corr_recall[i][j] = exact_ranks[i][0] in sym_corr_rank[:r]\n",
    "\n",
    "    # Catch the warning risen when the number of retrieved items for all the\n",
    "    # queries is smaller than some value of R\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        asym_recall_mean = np.nanmean(asym_recall, axis=0)\n",
    "        nearest_asym_recall_mean = np.nanmean(nearest_asym_recall, axis=0)\n",
    "        asym_corr_recall_mean = np.nanmean(asym_corr_recall, axis=0)\n",
    "        nearest_asym_corr_recall_mean = np.nanmean(nearest_asym_corr_recall, axis=0)\n",
    "        sym_recall_mean = np.nanmean(sym_recall, axis=0)\n",
    "        nearest_sym_recall_mean = np.nanmean(nearest_sym_recall, axis=0)\n",
    "        sym_corr_recall_mean = np.nanmean(sym_corr_recall, axis=0)\n",
    "        nearest_sym_corr_recall_mean = np.nanmean(nearest_sym_corr_recall, axis=0)\n",
    "        if w:\n",
    "            for warning in w:\n",
    "                if \"Mean of empty slice\" not in str(warning.message):\n",
    "                    warnings.warn(warning.message, warning.category)\n",
    "\n",
    "    results = {\n",
    "        \"asym_recall_mean\": asym_recall_mean,\n",
    "        \"nearest_asym_recall_mean\": nearest_asym_recall_mean,\n",
    "        \"asym_corr_recall_mean\": asym_corr_recall_mean,\n",
    "        \"nearest_asym_corr_recall_mean\": nearest_asym_corr_recall_mean,\n",
    "        \"sym_recall_mean\": sym_recall_mean,\n",
    "        \"nearest_sym_recall_mean\": nearest_sym_recall_mean,\n",
    "        \"sym_corr_recall_mean\": sym_corr_recall_mean,\n",
    "        \"nearest_sym_corr_recall_mean\": nearest_sym_corr_recall_mean,\n",
    "        \"inertia\": index.inertia\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the average recall at various values of R for the set of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "results_pq = compute_recall(pq, R, queries, exact_ranks)\n",
    "results_pq[\"search_NMSE\"] = NMSE(search_data, pq.decompress(pq.pqcode))\n",
    "del pq\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R, results_pq[\"nearest_asym_recall_mean\"], '-s', label=f\"ADC ({np.mean(results_pq['nearest_asym_recall_mean']):.2f})\", color=tab20(0))\n",
    "axs[0].plot(R, results_pq[\"nearest_asym_corr_recall_mean\"], '-D', label=f\"Corrected ADC ({np.mean(results_pq['nearest_asym_corr_recall_mean']):.2f})\", color=tab20(1))\n",
    "axs[0].plot(R, results_pq[\"nearest_sym_recall_mean\"], '-o', label=f\"SDC ({np.mean(results_pq['nearest_sym_recall_mean']):.2f})\", color=tab20(6))\n",
    "axs[0].plot(R, results_pq[\"nearest_sym_corr_recall_mean\"], '-p', label=f\"Corrected SDC ({np.mean(results_pq['nearest_sym_corr_recall_mean']):.2f})\", color=tab20(7))\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title=\"Method (Mean nearest recall@R)\")\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(R, results_pq[\"asym_recall_mean\"], '-s', label=f\"ADC ({np.mean(results_pq['asym_recall_mean']):.2f})\", color=tab20(0))\n",
    "axs[1].plot(R, results_pq[\"asym_corr_recall_mean\"], '-D', label=f\"Corrected ADC ({np.mean(results_pq['asym_corr_recall_mean']):.2f})\", color=tab20(1))\n",
    "axs[1].plot(R, results_pq[\"sym_recall_mean\"], '-o', label=f\"SDC ({np.mean(results_pq['sym_recall_mean']):.2f})\", color=tab20(6))\n",
    "axs[1].plot(R, results_pq[\"sym_corr_recall_mean\"], '-p', label=f\"Corrected SDC ({np.mean(results_pq['sym_corr_recall_mean']):.2f})\", color=tab20(7))\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].legend(title=\"Method (Mean recall@R)\")\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle(f\"m={M}, k*={K}\\n({tr_data.shape[1]//M} dims per subspace)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the average recall at 100 for the set of queries with different values of M and K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms = [1, 2, 4, 10, 20] if DATASET == \"glove\" else [1, 2, 4, 8, 16]\n",
    "Ks = [16, 64, 256, 1024]\n",
    "\n",
    "training_results = []\n",
    "search_results = []\n",
    "\n",
    "for m in Ms:\n",
    "    for k in Ks:\n",
    "        print(f\"Training PQ with m={m}, k*={k}...\")\n",
    "        \n",
    "        pq = PQ(M=m, K=k, seed=RANDOM_SEED)\n",
    "        start_training = time.time()\n",
    "        pq.train(tr_data, add=False)\n",
    "        training_time = time.time() - start_training\n",
    "        start_adding = time.time()\n",
    "        pq.add(search_data)\n",
    "        adding_time = time.time() - start_adding\n",
    "        \n",
    "        decompressed_search = pq.decompress(pq.pqcode) \n",
    "        search_NMSE = NMSE(search_data, decompressed_search)\n",
    "        compressed_tr = pq.compress(tr_data)\n",
    "        decompressed_tr = pq.decompress(compressed_tr)\n",
    "        tr_NMSE = NMSE(tr_data, decompressed_tr)\n",
    "\n",
    "        code_length = int(np.log2(k)) * m\n",
    "        curr_training_res = {\n",
    "            \"m\": m, \"k*\": k, \"Code length [bits]\": code_length,\n",
    "            \"Training time [s]\": training_time, \"Adding time [s]\": adding_time,\n",
    "            \"TR NMSE\": tr_NMSE,\n",
    "            \"SEARCH NMSE\": search_NMSE\n",
    "        }\n",
    "        training_results.append(curr_training_res)\n",
    "        \n",
    "        for asym in [True, False]:\n",
    "            mean_search_time = 0\n",
    "            nearest_recall_tr = 0\n",
    "            for i, query in enumerate(queries):\n",
    "                start_search = time.time()\n",
    "                _, ranking = pq.search(query, subset=None, asym=asym, correct=False)\n",
    "                mean_search_time += (time.time() - start_search)\n",
    "                if exact_ranks[i][0] in ranking[:100]:\n",
    "                    nearest_recall_tr += 1\n",
    "            mean_search_time /= queries.shape[0]\n",
    "            nearest_recall_tr /= queries.shape[0]\n",
    "            \n",
    "            curr_search_res = {\n",
    "                \"m\": m, \"k*\": k, \"Code length [bits]\": code_length,\n",
    "                \"Asymmetric Distance\": asym,\n",
    "                \"Mean search time [ms]\": mean_search_time*1000,\n",
    "                \"Nearest recall@100\": nearest_recall_tr\n",
    "            }\n",
    "            search_results.append(curr_search_res)\n",
    "\n",
    "del pq\n",
    "training_results = pd.DataFrame(training_results)\n",
    "search_results = pd.DataFrame(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the results about the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results.style.background_gradient(\n",
    "    subset=[\"Training time [s]\", \"Adding time [s]\", \"TR NMSE\", \"SEARCH NMSE\"],\n",
    "    cmap='Blues'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the reconstruction error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:red', 'tab:blue', 'tab:gray', 'tab:green', 'black']\n",
    "markers = ['+', 'x', 'o', 's', '^', 'D']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "for i, m in enumerate(Ms):\n",
    "    res = training_results[training_results[\"m\"]==m]\n",
    "    axs[0].plot(res[\"Code length [bits]\"], res[\"TR NMSE\"], marker=markers[i], color=colors[i], label=f\"m={m}\")\n",
    "    axs[1].plot(res[\"Code length [bits]\"], res[\"SEARCH NMSE\"], marker=markers[i], color=colors[i], label=f\"m={m}\")\n",
    "\n",
    "if DATASET != \"glove\":\n",
    "    axs[0].set_xticks([16, 32, 64, 96, 128, 160])\n",
    "    axs[1].set_xticks([16, 32, 64, 96, 128, 160])\n",
    "\n",
    "axs[0].set_xlabel(\"Code length (bits)\")\n",
    "axs[0].set_ylabel(\"Reconstruction error (NMSE)\")\n",
    "axs[0].set_title(\"Training data\")\n",
    "axs[0].legend()\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].set_xlabel(\"Code length (bits)\")\n",
    "axs[1].set_ylabel(\"Reconstruction error (NMSE)\")\n",
    "axs[1].set_title(\"Search data\")\n",
    "axs[1].legend()\n",
    "axs[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed number of bits, it is better to use a small number of subquantizers with many centroids than having many subquantizers with few bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the results about the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Exact Search time: {mean_es_time*1000} [ms]\")\n",
    "search_results.style.background_gradient(\n",
    "    subset=[\"Mean search time [ms]\", \"Nearest recall@100\"],\n",
    "    cmap='Blues'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the average recall at 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "for i, m in enumerate(Ms):\n",
    "    asym_res = search_results[(search_results[\"m\"]==m) & (search_results[\"Asymmetric Distance\"]==True)]\n",
    "    axs[0].plot(asym_res[\"Code length [bits]\"], asym_res[\"Nearest recall@100\"], marker=markers[i], color=colors[i], label=f\"m={m}\")\n",
    "    sym_res = search_results[(search_results[\"m\"]==m) & (search_results[\"Asymmetric Distance\"]==False)]\n",
    "    axs[1].plot(sym_res[\"Code length [bits]\"], sym_res[\"Nearest recall@100\"], marker=markers[i], color=colors[i], label=f\"m={m}\")\n",
    "\n",
    "if DATASET != \"glove\":\n",
    "    axs[0].set_xticks([16, 32, 64, 96, 128, 160])\n",
    "    axs[1].set_xticks([16, 32, 64, 96, 128, 160])\n",
    "\n",
    "axs[0].set_xlabel(\"Code length (bits)\")\n",
    "axs[0].set_ylabel(\"Nearest recall@100\")\n",
    "axs[0].set_title(\"ADC\")\n",
    "axs[0].legend()\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].set_xlabel(\"Code length (bits)\")\n",
    "axs[1].set_ylabel(\"Nearest recall@100\")\n",
    "axs[1].set_title(\"SDC\")\n",
    "axs[1].legend()\n",
    "axs[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asymmetric estimator ADC significantly outperforms SDC. On the \"siftsmall\" dataset, for $m=8$, ADC achieves higher accuracy with $k=64$ compared to SDC with $k=256$. Given that the efficiency of the two approaches is equivalent, the authors advocate not to quantize the query when possible, but only the database elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the search time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "for i, m in enumerate(Ms):\n",
    "    asym_res = search_results[(search_results[\"m\"]==m) & (search_results[\"Asymmetric Distance\"]==True)]\n",
    "    axs[0].plot(asym_res[\"Code length [bits]\"], asym_res[\"Mean search time [ms]\"], marker=markers[i], color=colors[i], label=f\"m={m}\")\n",
    "    sym_res = search_results[(search_results[\"m\"]==m) & (search_results[\"Asymmetric Distance\"]==False)]\n",
    "    axs[1].plot(sym_res[\"Code length [bits]\"], sym_res[\"Mean search time [ms]\"], marker=markers[i], color=colors[i], label=f\"m={m}\")\n",
    "\n",
    "axs[0].axhline(mean_es_time*1000, 0, 32, color='black', linestyle='--', label=\"Exact search\")\n",
    "axs[1].axhline(mean_es_time*1000, 0, 32, color='black', linestyle='--', label=\"Exact search\")\n",
    "\n",
    "if DATASET == \"siftsmall\":\n",
    "    axs[0].set_xticks([16, 32, 64, 96, 128, 160])\n",
    "    axs[1].set_xticks([16, 32, 64, 96, 128, 160])\n",
    "\n",
    "axs[0].set_xlabel(\"Code length (bits)\")\n",
    "axs[0].set_ylabel(\"Mean search time [ms]\")\n",
    "axs[0].set_title(\"ADC\")\n",
    "axs[0].legend()\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].set_xlabel(\"Code length (bits)\")\n",
    "axs[1].set_ylabel(\"Mean search time [ms]\")\n",
    "axs[1].set_title(\"SDC\")\n",
    "axs[1].legend()\n",
    "axs[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT descriptors are constructed as concatenated orientation histograms. Each histogram is calculated within grid cells of an image patch, as illustrated in the figure below (sourced from \"Computer Vision Metrics - Survey, Taxonomy, and Analysis\", by Scott Krigg).\n",
    "\n",
    "<img src=\"./img/sift.png\" alt=\"My Image\" width=\"500\"/>\n",
    "\n",
    "Using a product quantizer, histogram bins may be assigned to different quantization groups. The \"natural\" order groups consecutive components, while the \"structured\" order clusters related dimensions—for example, grouping 2x2 patches when M=4. To assess the impact of these grouping strategies, we also compare them to a \"random\" order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"siftsmall\":\n",
    "    # Prepare data for the structured permutation\n",
    "    struct_perm = list(range(0, 16)) + list(range(32, 48)) + \\\n",
    "        list(range(16, 32)) + list(range(48, 64)) + \\\n",
    "        list(range(64, 80)) + list(range(96, 112)) + \\\n",
    "        list(range(80, 96)) + list(range(112, 128))\n",
    "    tr_data_struct = tr_data[:, struct_perm]\n",
    "    queries_struct = queries[:, struct_perm]\n",
    "    search_data_struct = search_data[:, struct_perm]\n",
    "\n",
    "    # Compute recall for the structured permutation when training and searching\n",
    "    # on different data\n",
    "    pq_struct = PQ(M=4, K=K, seed=RANDOM_SEED)\n",
    "    print(\"Training PQ with structured permutation on training data...\")\n",
    "    pq_struct.train(tr_data_struct, add=False)\n",
    "    pq_struct.add(search_data_struct)\n",
    "    results_struct_disj = compute_recall(\n",
    "        index=pq_struct, R=[10], queries=queries_struct,\n",
    "        exact_ranks=exact_ranks, correct=False,\n",
    "        sym=False\n",
    "    )\n",
    "    recall_struct_perm_disj = results_struct_disj[\"nearest_asym_recall_mean\"][0]\n",
    "    del pq_struct\n",
    "\n",
    "    # Compute recall for the natural permutation when training and searching\n",
    "    # on different data\n",
    "    pq = PQ(M=4, K=K, seed=RANDOM_SEED)\n",
    "    print(\"Training PQ with natural permutation on training data...\")\n",
    "    pq.train(tr_data, add=False)\n",
    "    pq.add(search_data)\n",
    "    results_disj = compute_recall(\n",
    "        index=pq, R=[10], queries=queries,\n",
    "        exact_ranks=exact_ranks, correct=False,\n",
    "        sym=False\n",
    "    )\n",
    "    recall_disj = results_disj[\"nearest_asym_recall_mean\"][0]\n",
    "    del pq\n",
    "\n",
    "    recalls_rand_disj = []\n",
    "    for rep in range(0, 5):\n",
    "        # Prepare data for the random permutation\n",
    "        rand_perm = np.random.permutation(128)\n",
    "        tr_data_rand = tr_data[:, rand_perm]\n",
    "        queries_rand = queries[:, rand_perm]\n",
    "        search_data_rand = search_data[:, rand_perm]\n",
    "\n",
    "        # Compute recall for the random permutation when training and searching\n",
    "        # on different data\n",
    "        pq_rand = PQ(M=4, K=K, seed=RANDOM_SEED)\n",
    "        print(f\"[{rep+1}/5] Training PQ with random permutation on training data...\")\n",
    "        pq_rand.train(tr_data_rand, add=False)\n",
    "        pq_rand.add(search_data_rand)\n",
    "        results_rand_disj = compute_recall(\n",
    "            index=pq_rand, R=[10], queries=queries_rand,\n",
    "            exact_ranks=exact_ranks, correct=False,\n",
    "            sym=False\n",
    "        )\n",
    "        recall_rand_disj = results_rand_disj[\"nearest_asym_recall_mean\"][0]\n",
    "        recalls_rand_disj.append(recall_rand_disj)\n",
    "\n",
    "    del pq_rand\n",
    "    recall_rand_mean_disj = np.mean(recalls_rand_disj)\n",
    "    recall_rand_std_disj = np.std(recalls_rand_disj)\n",
    "\n",
    "    # Compute recall for the structured permutation when training and searching\n",
    "    # on the same data\n",
    "    pq_struct = PQ(M=4, K=K, seed=RANDOM_SEED)\n",
    "    print(\"Training PQ with structured permutation on search data...\")\n",
    "    pq_struct.train(search_data_struct, add=True)\n",
    "    results_perm = compute_recall(\n",
    "        index=pq_struct, R=[10], queries=queries_struct,\n",
    "        exact_ranks=exact_ranks, correct=False, sym=False\n",
    "    )\n",
    "    recall_struct = results_perm[\"nearest_asym_recall_mean\"][0]\n",
    "\n",
    "    # Compute recall for the natural permutation when training and searching\n",
    "    # on the same data\n",
    "    print(\"Training PQ with natural permutation on search data...\")\n",
    "    pq = PQ(M=4, K=K, seed=RANDOM_SEED)\n",
    "    pq.train(search_data, add=True)\n",
    "    results_search = compute_recall(\n",
    "        index=pq, R=[10], queries=queries,\n",
    "        exact_ranks=exact_ranks, correct=False, sym=False\n",
    "    )\n",
    "    recall_search = results_search[\"nearest_asym_recall_mean\"][0]\n",
    "\n",
    "    recalls_rand = []\n",
    "    for rep in range(0, 5):\n",
    "        # Prepare data for the random permutation\n",
    "        rand_perm = np.random.permutation(128)\n",
    "        tr_data_rand = tr_data[:, rand_perm]\n",
    "        queries_rand = queries[:, rand_perm]\n",
    "        search_data_rand = search_data[:, rand_perm]\n",
    "        \n",
    "        # Compute recall for the random permutation when training and searching\n",
    "        # on the same data\n",
    "        print(f\"[{rep+1}/5] Training PQ with random permutation on search data...\")\n",
    "        pq_rand = PQ(M=4, K=K, seed=RANDOM_SEED)\n",
    "        pq_rand.train(search_data_rand, add=True)\n",
    "        results_random = compute_recall(\n",
    "            index=pq_rand, R=[10], queries=queries_rand,\n",
    "            exact_ranks=exact_ranks, correct=False, sym=False\n",
    "        )\n",
    "        recall_rand = results_random[\"nearest_asym_recall_mean\"][0]\n",
    "        recalls_rand.append(recall_rand)\n",
    "\n",
    "    del pq_rand\n",
    "    recall_rand_mean = np.mean(recalls_rand)\n",
    "    recall_rand_std = np.std(recalls_rand)\n",
    "\n",
    "    display(pd.DataFrame({\n",
    "        \"natural\": [nearest_recall_tr, recall_search],\n",
    "        \"structured\": [recall_struct_perm_disj, recall_struct],\n",
    "        \"random mean\": [recall_rand_mean_disj, recall_rand_mean],\n",
    "        \"random std\": [recall_rand_std_disj, recall_rand_std]\n",
    "    }, index=[\n",
    "        \"nearest recall@10 (training data)\",\n",
    "        \"nearest recall@10 (search data)\"\n",
    "        ]\n",
    "    ).style.background_gradient(cmap='Blues', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the impact of KMeans initialization on the search performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PQ_recall(M, K, seed, R, tr_data, search_data, queries, exact_ranks):\n",
    "    pq = PQ(M=M, K=K, seed=seed)\n",
    "    pq.train(tr_data, add=False)\n",
    "    pq.add(search_data, compute_distortions=False)\n",
    "    results = compute_recall(pq, R, queries, exact_ranks, correct=False, sym=False)\n",
    "    return results\n",
    "\n",
    "R_small = [1, 5, 10, 15, 20, 25]\n",
    "\n",
    "results_pq_seeds = []\n",
    "for i in range(5):\n",
    "    results = compute_PQ_recall(M, K, RANDOM_SEED+i, R_small, tr_data, search_data, queries, exact_ranks)\n",
    "    results_pq_seeds.append(results)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "nearest_asym_recall_means_seeds = np.array([results[\"nearest_asym_recall_mean\"] for results in results_pq_seeds])\n",
    "mean_nearest_seeds = nearest_asym_recall_means_seeds.mean(axis=0)\n",
    "min_nearest_seeds = nearest_asym_recall_means_seeds.min(axis=0)\n",
    "max_nearest_seeds = nearest_asym_recall_means_seeds.max(axis=0)\n",
    "\n",
    "axs[0].plot(R_small, mean_nearest_seeds, marker='o', label='Mean')\n",
    "axs[0].fill_between(R_small, min_nearest_seeds, max_nearest_seeds, alpha=0.2, label='Min-Max Range')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].grid()\n",
    "axs[0].legend()\n",
    "\n",
    "asym_recall_means_seeds = np.array([results[\"asym_recall_mean\"] for results in results_pq_seeds])\n",
    "mean_asym_seeds = asym_recall_means_seeds.mean(axis=0)\n",
    "min_asym_seeds = asym_recall_means_seeds.min(axis=0)\n",
    "max_asym_seeds = asym_recall_means_seeds.max(axis=0)\n",
    "\n",
    "axs[1].plot(R_small, mean_asym_seeds, marker='o', label='Mean')\n",
    "axs[1].fill_between(R_small, min_asym_seeds, max_asym_seeds, alpha=0.2, label='Min-Max Range')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].grid()\n",
    "axs[1].legend()\n",
    "\n",
    "fig.suptitle(f\"ADC, m={M}, k*={K} with 5 different seeds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = np.std([results[\"nearest_asym_recall_mean\"] for results in results_pq_seeds], axis=0)\n",
    "seeds_res_df = pd.DataFrame(stds, index=R_small, columns=[\"Standard deviation of nearest recall @ R\"])\n",
    "seeds_res_df.index.name = \"R\"\n",
    "seeds_res_df.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"siftsmall\":\n",
    "    fig, axs = plt.subplots(1, M, figsize=(15, 4))\n",
    "\n",
    "    for i in range(M):\n",
    "        for j, results in enumerate(results_pq_seeds):\n",
    "            axs[i].scatter(i, results[\"inertia\"][i], color=tab10(j), marker=markers[j])\n",
    "            axs[i].set_xticks([])\n",
    "            axs[i].set_xlabel(f\"Subspace {i+1}\")\n",
    "\n",
    "    fig.suptitle(\"Inertia for different seeds and subspaces\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Quantization with InVerted File index (IVF)\n",
    "\n",
    "**Implementation details**\n",
    "\n",
    "Rather than storing the vector ID and its code in the inverted list, as shown in the following figure from the original article, we store only the row index of the vector in the database's matrix of vectors. During a search, we retrieve the inverted index lists whose centroids are closest to the query vector and limit the search to those vectors by passing their indices to the PQ class's search method.\n",
    "\n",
    "<img src=\"./img/ivf.png\" width=\"400\">\n",
    "\n",
    "Training the PQ quantizer with IVF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KP = 128\n",
    "W = 8\n",
    "\n",
    "ivf = IVF(Kp=KP, M=M, K=K, seed=RANDOM_SEED)\n",
    "start_training_ivf = time.time()\n",
    "ivf.train(tr_data, add=False, verbose=True)\n",
    "training_ivf = time.time() - start_training_ivf\n",
    "\n",
    "start_adding_ivf = time.time()\n",
    "ivf.add(search_data, compute_distortions=True)\n",
    "adding_ivf = time.time() - start_adding_ivf\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Training time [s]\": training_ivf,\n",
    "    \"Adding time [s]\": adding_ivf,\n",
    "}, index=[\"With average distance computation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the index lists lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "els_per_cluster = []\n",
    "for i in range(ivf.Kp):\n",
    "    els_per_cluster.append(len(ivf.ivf[i]))\n",
    "fig, axs = plt.subplots(1)\n",
    "axs.bar(range(ivf.Kp), sorted(els_per_cluster, reverse=True), width=1.0)\n",
    "axs.set_xlabel(\"Lists (sorted by length)\")\n",
    "axs.set_ylabel(\"Length\")\n",
    "axs.set_title(\"Vectors per list\")\n",
    "axs.set_axisbelow(True)\n",
    "axs.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the average recall at various values of R for the set of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ivf = compute_recall(ivf, R, queries, exact_ranks, w=W)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R, results_ivf[\"nearest_asym_recall_mean\"], '-s', label=f\"IVFADC ({np.nanmean(results_ivf['nearest_asym_recall_mean']):.2f})\", color=tab20(0))\n",
    "axs[0].plot(R, results_ivf[\"nearest_asym_corr_recall_mean\"], '-D', label=f\"Corrected IVFADC ({np.nanmean(results_ivf['nearest_asym_corr_recall_mean']):.2f})\", color=tab20(1))\n",
    "axs[0].plot(R, results_ivf[\"nearest_sym_recall_mean\"], '-o', label=f\"IVFSDC ({np.nanmean(results_ivf['nearest_sym_recall_mean']):.2f})\", color=tab20(6))\n",
    "axs[0].plot(R, results_ivf[\"nearest_sym_corr_recall_mean\"], '-p', label=f\"Corrected IVFSDC ({np.nanmean(results_ivf['nearest_sym_corr_recall_mean']):.2f})\", color=tab20(7))\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title=\"Method (Mean nearest recall@R)\")\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(R, results_ivf[\"asym_recall_mean\"], '-s', label=f\"IVFADC ({np.nanmean(results_ivf['asym_recall_mean']):.2f})\", color=tab20(0))\n",
    "axs[1].plot(R, results_ivf[\"asym_corr_recall_mean\"], '-D', label=f\"Corrected IVFADC ({np.nanmean(results_ivf['asym_corr_recall_mean']):.2f})\", color=tab20(1))\n",
    "axs[1].plot(R, results_ivf[\"sym_recall_mean\"], '-o', label=f\"IVFSDC ({np.nanmean(results_ivf['sym_recall_mean']):.2f})\", color=tab20(6))\n",
    "axs[1].plot(R, results_ivf[\"sym_corr_recall_mean\"], '-p', label=f\"Corrected IVFSDC ({np.nanmean(results_ivf['sym_corr_recall_mean']):.2f})\", color=tab20(7))\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].legend(title=\"Method (Mean recall@R)\")\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle(f\"m={M}, k*={K}, k'={KP}, w={W}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the average recall at 100 of PQ with and without IVF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R, results_ivf[\"nearest_asym_recall_mean\"], '-s', label=f\"IVFADC, k'={KP}, w={W} ({np.nanmean(results_ivf['nearest_asym_recall_mean']):.2f})\", color=tab20(0))\n",
    "axs[0].plot(R, results_ivf[\"nearest_sym_recall_mean\"], '-D', label=f\"IVFSDC, k'={KP}, w={W} ({np.nanmean(results_ivf['nearest_sym_recall_mean']):.2f})\", color=tab20(6))\n",
    "axs[0].plot(R, results_pq[\"nearest_asym_recall_mean\"], '-o', label=f\"ADC ({np.nanmean(results_pq['nearest_asym_recall_mean']):.2f})\", color=tab20(4))\n",
    "axs[0].plot(R, results_pq[\"nearest_sym_recall_mean\"], '-p', label=f\"SDC ({np.nanmean(results_pq['nearest_sym_recall_mean']):.2f})\", color=tab20(8))\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title='Method (Mean nearest recall@R)')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(R, results_ivf[\"asym_recall_mean\"], '-s', label=f\"IVFADC, k'={KP}, w={W} ({np.nanmean(results_ivf['asym_recall_mean']):.2f})\", color=tab20(0))\n",
    "axs[1].plot(R, results_ivf[\"sym_recall_mean\"], '-D', label=f\"IVFSDC, k'={KP}, w={W} ({np.nanmean(results_ivf['sym_recall_mean']):.2f})\", color=tab20(6))\n",
    "axs[1].plot(R, results_pq[\"asym_recall_mean\"], '-o', label=f\"ADC ({np.nanmean(results_pq['asym_recall_mean']):.2f})\", color=tab20(4))\n",
    "axs[1].plot(R, results_pq[\"sym_recall_mean\"], '-p', label=f\"SDC ({np.nanmean(results_pq['sym_recall_mean']):.2f})\", color=tab20(8))\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].legend(title='Method (Mean recall@R)')\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle(f\"m={M}, k*={K}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Inverted File Index structure, curves stop at some point, as only a fraction of the database vectors are ranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the average distances from the queries to the coarse centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dists2centroids = np.zeros(ivf.Kp)\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    _, ivf_asym_rank = ivf.search(query, w=W, asym=True, correct=False)\n",
    "    dist2centroids = cdist([query], ivf.centroids, 'euclidean')[0]\n",
    "    sorted_dist2centroids = np.sort(dist2centroids)\n",
    "    mean_dists2centroids += sorted_dist2centroids\n",
    "\n",
    "mean_dists2centroids /= queries.shape[0]\n",
    "\n",
    "plt.plot(mean_dists2centroids)\n",
    "plt.xlabel(\"Centroids sorted by distance\")\n",
    "plt.ylabel(\"Euclidean Distance\")\n",
    "plt.axvline(x=W, color='k', linestyle='--', label=f\"w={W}\")\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title(f\"Average distance between queries and coarse centroids\\n(m={M}, k*={K}, k'={KP}, w={W})\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the average recall at 100 for the set of queries with different values of M, K' and w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 256\n",
    "Ms = [1, 2, 4, 10, 20] if DATASET == \"glove\" else [1, 2, 4, 8, 16]\n",
    "Kps = [16, 128]\n",
    "ws = [1, 2, 8]\n",
    "\n",
    "ivf_training_results = []\n",
    "ivf_search_results = []\n",
    "\n",
    "for m in Ms:\n",
    "    for Kp in Kps:\n",
    "        print(f\"Training IVF PQ with M={m}, k'={Kp}...\")\n",
    "        ivf = IVF(Kp=Kp, M=m, K=K, seed=RANDOM_SEED)\n",
    "        start_training = time.time()\n",
    "        ivf.train(tr_data, add=False)\n",
    "        training_time = time.time() - start_training\n",
    "        start_adding = time.time()\n",
    "        ivf.add(search_data)\n",
    "        adding_time = time.time() - start_adding\n",
    "\n",
    "        code_length = int(np.log2(K)) * m\n",
    "        curr_training_res = {\n",
    "            \"m\": m, \"k*\": K, \"k'\": Kp,\n",
    "            \"Code length [bits]\": code_length,\n",
    "            \"Training time [s]\": training_time,\n",
    "            \"Adding time [s]\": adding_time\n",
    "        }\n",
    "        ivf_training_results.append(curr_training_res)\n",
    "        \n",
    "        for w in ws:\n",
    "            if (Kp == Kps[0] and w == ws[-1]) or (Kp == Kps[-1] and w == ws[0]):\n",
    "                continue\n",
    "            mean_search_time = 0\n",
    "            nearest_recall_tr = 0\n",
    "            for i, query in enumerate(queries):\n",
    "                start_search = time.time()\n",
    "                _, ranking = ivf.search(query, w=w, asym=True, correct=False)\n",
    "                mean_search_time += (time.time() - start_search)\n",
    "                if exact_ranks[i][0] in ranking[:100]:\n",
    "                    nearest_recall_tr += 1\n",
    "            mean_search_time /= len(queries)\n",
    "            nearest_recall_tr /= len(queries)\n",
    "            \n",
    "            curr_search_res = {\n",
    "                \"m\": m, \"k*\": K, \"k'\": Kp, \"w\": w,\n",
    "                \"Code length [bits]\": code_length,\n",
    "                \"Mean search time [ms]\": mean_search_time*1000,\n",
    "                \"Nearest recall@100\": nearest_recall_tr\n",
    "            }\n",
    "            ivf_search_results.append(curr_search_res)\n",
    "\n",
    "del ivf\n",
    "ivf_training_results = pd.DataFrame(ivf_training_results)\n",
    "ivf_search_results = pd.DataFrame(ivf_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the average recall at 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:red', 'tab:blue', 'tab:gray', 'tab:green', 'black']\n",
    "i = 0\n",
    "for Kp in Kps:\n",
    "    for w in ws:\n",
    "        if (Kp == Kps[0] and w == ws[-1]) or (Kp == Kps[-1] and w == ws[0]):\n",
    "            continue\n",
    "        res = ivf_search_results[(ivf_search_results[\"k'\"]==Kp) & (ivf_search_results[\"w\"]==w)]\n",
    "        plt.plot(res[\"Code length [bits]\"], res[\"Nearest recall@100\"], marker=markers[i], color=colors[i], label=f\"k'={Kp}, w={w}\")\n",
    "        i += 1\n",
    "\n",
    "plt.xlabel(\"Code length (bits)\")\n",
    "plt.ylabel(\"Nearest recall@100\")\n",
    "plt.title(f\"IVFADC\\n(k*={K}, m in {Ms})\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest recall@100 strongly depends on the IVF parameters. Increasing the code length is useless if $w$ is not big enough, as the nearest neighbors which are not assigned to one of the $w$ centroids associated with the query are definitely lost.\n",
    "\n",
    "This approach is significantly more efficient than SDC and ADC on large datasets, as it only compares the query to a small fraction of the database vectors. The proportion of the dataset to visit is roughly linear in $w/k'$.\n",
    "\n",
    "For a fixed proportion, it is worth using higher values of $k'$, as this increases the accuracy, as shown, on the \"sift\" dataset, by comparing, for the tuple (k', w), the parameters (1024, 1) against (8192, 8) and (1024, 8) against (8192, 64)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the search time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for Kp in Kps:\n",
    "    for w in ws:\n",
    "        if (Kp == Kps[0] and w == ws[-1]) or (Kp == Kps[-1] and w == ws[0]):\n",
    "            continue\n",
    "        res = ivf_search_results[(ivf_search_results[\"k'\"]==Kp) & (ivf_search_results[\"w\"]==w)]\n",
    "        plt.plot(res[\"Code length [bits]\"], res[\"Mean search time [ms]\"], marker=markers[i], color=colors[i], label=f\"k'={Kp}, w={w}\")\n",
    "        i += 1\n",
    "\n",
    "plt.axhline(mean_es_time*1000, 0, 32, color='black', linestyle='--', label=\"Exact search\")\n",
    "plt.xlabel(\"Code length (bits)\")\n",
    "plt.ylabel(\"Mean search time [ms]\")\n",
    "plt.title(f\"IVFADC\\n(k*={K}, m in {Ms})\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher values of $k'$ yield higher search efficiencies for large datasets, as the search benefits from parsing a smaller fraction of the memory. However, for small datasets, the complexity of the coarse quantizer may be the bottleneck if $k' \\times D > n/k'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing search times and average recall at 100 of PQ with and without IVF, for a fixed code length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10 if DATASET == \"glove\" else 8\n",
    "search_results[(search_results[\"m\"]==m) & (search_results[\"k*\"]==256)].style.background_gradient(\n",
    "    subset=['Mean search time [ms]', 'Nearest recall@100'],\n",
    "    cmap='Blues'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivf_search_results[(ivf_search_results[\"m\"]==m)].style.background_gradient(\n",
    "    subset=['Mean search time [ms]', 'Nearest recall@100'],\n",
    "    cmap='Blues'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of alternative approaches\n",
    "\n",
    "### Product Quantization\n",
    "\n",
    "Computing the average recall at various values of R for the set of queries on scaled data with different scaling approaches as well as the KMeans inertia (sum of squared distances of samples to their closest cluster center) in each subspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "tr_data_std = std_scaler.fit_transform(tr_data)\n",
    "search_data_std = std_scaler.transform(search_data)\n",
    "queries_std = std_scaler.transform(queries)\n",
    "es_std = ExactSearch(search_data_std)\n",
    "exact_ranks_std = np.empty((queries_std.shape[0], search_data_std.shape[0]))\n",
    "for i, query in enumerate(queries_std):\n",
    "    _, rank = es_std.search(query)\n",
    "    exact_ranks_std[i] = rank\n",
    "del es_std\n",
    "\n",
    "R_medium = [1, 2, 5, 10, 25, 50, 100]\n",
    "pq = PQ(M=M, K=K, seed=RANDOM_SEED)\n",
    "pq.train(tr_data, add=False, compute_distortions=True)\n",
    "pq.add(search_data)\n",
    "results_pq_med = compute_recall(pq, R_medium, queries, exact_ranks, correct=False, sym=True)\n",
    "\n",
    "pq_std = PQ(M=M, K=K, seed=RANDOM_SEED)\n",
    "pq_std.train(tr_data_std, add=False)\n",
    "pq_std.add(search_data_std, compute_distortions=True)\n",
    "results_pq_std = compute_recall(pq_std, R_medium, queries_std, exact_ranks_std)\n",
    "del pq_std\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "tr_data_minmax = minmax_scaler.fit_transform(tr_data)\n",
    "search_data_minmax = minmax_scaler.transform(search_data)\n",
    "queries_minmax = minmax_scaler.transform(queries)\n",
    "es_minmax = ExactSearch(search_data_minmax)\n",
    "exact_ranks_minmax = np.empty((queries_minmax.shape[0], search_data_minmax.shape[0]))\n",
    "for i, query in enumerate(queries_minmax):\n",
    "    _, rank = es_minmax.search(query)\n",
    "    exact_ranks_minmax[i] = rank\n",
    "del es_minmax\n",
    "\n",
    "pq_minmax = PQ(M=M, K=K, seed=RANDOM_SEED)\n",
    "pq_minmax.train(tr_data_minmax, add=False)\n",
    "pq_minmax.add(search_data_minmax, compute_distortions=True)\n",
    "results_pq_minmax = compute_recall(pq_minmax, R_medium, queries_minmax, exact_ranks_minmax)\n",
    "del pq_minmax\n",
    "\n",
    "scaling_methods = [\n",
    "    (\"Raw data\", results_pq_med),\n",
    "    (\"Standard scaling\", results_pq_std),\n",
    "    (\"Min-max scaling\", results_pq_minmax)\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "for ax, (title, data) in zip(axs, scaling_methods):\n",
    "    ax.plot(R_medium, data[\"nearest_asym_recall_mean\"], '-s', label=f\"ADC ({np.mean(data['nearest_asym_recall_mean']):.2f})\", color=tab20(0))\n",
    "    ax.plot(R_medium, data[\"nearest_asym_corr_recall_mean\"], '-D', label=f\"Corrected ADC ({np.mean(data['nearest_asym_corr_recall_mean']):.2f})\", color=tab20(1))\n",
    "    ax.plot(R_medium, data[\"nearest_sym_recall_mean\"], '-o', label=f\"SDC ({np.mean(data['nearest_sym_recall_mean']):.2f})\", color=tab20(6))\n",
    "    ax.plot(R_medium, data[\"nearest_sym_corr_recall_mean\"], '-p', label=f\"Corrected SDC ({np.mean(data['nearest_sym_corr_recall_mean']):.2f})\", color=tab20(7))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('R')\n",
    "    ax.set_ylabel('Nearest recall@R')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(title=\"Method (Mean nearest recall@R)\")\n",
    "    ax.grid()\n",
    "fig.suptitle(f\"m={M}, k*={K}\\n({tr_data.shape[1] // M} dims per subspace)\")\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1)\n",
    "\n",
    "colors = [\"black\", tab10(0), tab10(1)]\n",
    "markers = [\"--s\", \"-o\", \"-v\"]\n",
    "for i, (title, data) in enumerate(scaling_methods):\n",
    "    axs.plot(R_medium, data[\"nearest_asym_recall_mean\"], markers[i], label=f\"{title} ({np.mean(data['nearest_asym_recall_mean']):.2f})\", color=colors[i])\n",
    "axs.set_xscale('log')\n",
    "axs.set_xlabel('R')\n",
    "axs.set_ylabel('Nearest recall@R')\n",
    "axs.legend(title=\"Method (Mean nearest recall@R)\")\n",
    "axs.grid()\n",
    "fig.suptitle(f\"ADC, m={M}, k*={K}\\n({tr_data.shape[1] // M} dims per subspace)\")\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "for ax, (title, results) in zip(axs, scaling_methods):\n",
    "    ax.bar(range(1, results[\"inertia\"].shape[0]+1), results[\"inertia\"], zorder=3)\n",
    "    ax.set_xlabel('Subspace')\n",
    "    ax.set_ylabel('Inertia')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(zorder=0)\n",
    "fig.suptitle(f\"m={M}, k*={K}\\n({tr_data.shape[1] // M} dims per subspace)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the variance explained by the PCA components in each subspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_pca = PQ(M=M, K=K, seed=RANDOM_SEED)\n",
    "pq_pca.plot_variance_explained(tr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the average recall at various values of R for the set of queries on PCA-reduced data, using different numbers of components and the following approaches:\n",
    "- Subvectors are reduced in dimensionality, centroids are computed in this reduced space, and the reduced centroids are stored. At query time, query subvectors are also reduced before lookup. This is implemented by calling the PQ constructor with `dim_reduction=True` and specifying the number of dimensions when calling the `train` method. This approach is labelled \"Subspace Red.\" in the figure legend.\n",
    "- Subspaces are reduced in dimensionality, centroids are computed in the reduced space, but these centroids are transformed back to the original space and stored in full dimensionality. Queries are compared against the full-dimensional centroids. This is achieved by calling the PQ constructor with `dim_reduction=False` and specifying the number of dimensions when calling the `train` method. This approach is labelled \"Centroids Red.\" in the figure legend.\n",
    "- Entire vectors, rather than subspaces, are reduced in dimensionality before PQ training. This is labelled \"Full Vectors Red.\" in the figure legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pca = {}\n",
    "num_dims = [4, 6]\n",
    "num_dims.append(tr_data.shape[1]//M)\n",
    "for dim in num_dims:\n",
    "    pq_pca = PQ(M=M, K=K, seed=RANDOM_SEED, dim_reduction=True)\n",
    "    pq_pca.train(tr_data, add=False, num_dims=dim)\n",
    "    pq_pca.add(search_data, compute_distortions=False)\n",
    "    results_pq_pca = compute_recall(pq_pca, R_medium, queries, exact_ranks, correct=False, sym=False)\n",
    "    results_pca[f\"{dim}_red\"] = results_pq_pca\n",
    "    results_pca[f\"{dim}_red\"][\"search_NMSE\"] = NMSE(search_data, pq_pca.decompress(pq_pca.pqcode))\n",
    "\n",
    "for dim in num_dims:\n",
    "    pq_pca = PQ(M=M, K=K, seed=RANDOM_SEED, dim_reduction=False)\n",
    "    pq_pca.train(tr_data, add=False, num_dims=dim)\n",
    "    pq_pca.add(search_data, compute_distortions=False)\n",
    "    results_pq_pca = compute_recall(pq_pca, R_medium, queries, exact_ranks, correct=False, sym=False)\n",
    "    results_pca[dim] = results_pq_pca\n",
    "    results_pca[dim][\"search_NMSE\"] = NMSE(search_data, pq_pca.decompress(pq_pca.pqcode))\n",
    "\n",
    "n_components = M*6\n",
    "pca = PCA(n_components=n_components).fit(tr_data)\n",
    "tr_data_red = pca.transform(tr_data)\n",
    "search_data_red = pca.transform(search_data)\n",
    "queries_red = pca.transform(queries)\n",
    "pq_pca = PQ(M=M, K=K, seed=RANDOM_SEED)\n",
    "pq_pca.train(tr_data_red, add=False)\n",
    "pq_pca.add(search_data_red, compute_distortions=False)\n",
    "results_pq_pca = compute_recall(pq_pca, R_medium, queries_red, exact_ranks, correct=False, sym=False)\n",
    "results_pca[\"full_red\"] = results_pq_pca\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R_medium, results_pq_med[\"nearest_asym_recall_mean\"], '--s', label=f\"No Red. ({np.mean(results_pq_med['nearest_asym_recall_mean']):.2f})\", color=\"black\")\n",
    "for i, dim, m1, m2 in zip(range(0, len(num_dims)*2+2, 2), num_dims, ['-o', '-X', '-v'], ['-p', '-P', '-^']):\n",
    "    axs[0].plot(R_medium, results_pca[f\"{dim}_red\"][\"nearest_asym_recall_mean\"], m2, label=f\"Subspace Red., {dim} dims ({np.mean(results_pca[f'{dim}_red']['nearest_asym_recall_mean']):.2f})\", color=tab20(i+1))\n",
    "    axs[0].plot(R_medium, results_pca[dim][\"nearest_asym_recall_mean\"], m1, label=f\"Centroids Red., {dim} dims ({np.mean(results_pca[dim]['nearest_asym_recall_mean']):.2f})\", color=tab20(i))\n",
    "axs[0].plot(R_medium, results_pca[\"full_red\"][\"nearest_asym_recall_mean\"], '-D', label=f\"Full Vectors Red., {n_components} dims ({np.mean(results_pca['full_red']['nearest_asym_recall_mean']):.2f})\", color=tab20(len(num_dims)*2))\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title='Method (Mean nearest recall@R)')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(R_medium, results_pq_med[\"asym_recall_mean\"], '--s', label=f\"No. Red. ({np.mean(results_pq_med['asym_recall_mean']):.2f})\", color=\"black\")\n",
    "for i, dim, m1, m2 in zip(range(0, len(num_dims)*2+2, 2), num_dims, ['-o', '-X', '-v'], ['-p', '-P', '-^']):\n",
    "    axs[1].plot(R_medium, results_pca[f\"{dim}_red\"][\"asym_recall_mean\"], m2, label=f\"Subspace Red., {dim} dims ({np.mean(results_pca[f'{dim}_red']['asym_recall_mean']):.2f})\", color=tab20(i+1))\n",
    "    axs[1].plot(R_medium, results_pca[dim][\"asym_recall_mean\"], m1, label=f\"Centroids Red., {dim} dims ({np.mean(results_pca[dim]['asym_recall_mean']):.2f})\", color=tab20(i))\n",
    "axs[1].plot(R_medium, results_pca[\"full_red\"][\"asym_recall_mean\"], '-D', label=f\"Full Vectors Red., {n_components} dims ({np.mean(results_pca['full_red']['asym_recall_mean']):.2f})\", color=tab20(len(num_dims)*2))\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].legend(title='Method (Mean recall@R)')\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle(f\"ADC, m={M}, k*={K}\\n({tr_data.shape[1]//M} dims per supspace)\");\n",
    "\n",
    "del pq_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop in recall is due to training-indexing mismatch.\n",
    "\n",
    "Displaying NMSE on search data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nmse = {}\n",
    "pca_nmse[\"No Dim. Red.\"] = results_pq[\"search_NMSE\"]\n",
    "for dim in num_dims:\n",
    "    pca_nmse[f\"Centroids Dim. Red., {dim} dims\"] = results_pca[dim][\"search_NMSE\"]\n",
    "    pca_nmse[f\"Subspace Dim. Red., {dim} dims\"] = results_pca[f\"{dim}_red\"][\"search_NMSE\"]\n",
    "\n",
    "pd.DataFrame(pca_nmse, index=[\"NMSE on search data\"]).T.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying whitening to the best performing approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 6\n",
    "\n",
    "pq_pca = PQ(M=M, K=K, seed=RANDOM_SEED, dim_reduction=True)\n",
    "pq_pca.train(tr_data, add=False, num_dims=num_dims, whiten=True)\n",
    "pq_pca.add(search_data, compute_distortions=False)\n",
    "results_pq_whiten_red_pca = compute_recall(pq_pca, R_medium, queries, exact_ranks, correct=False, sym=False)\n",
    "\n",
    "pq_pca = PQ(M=M, K=K, seed=RANDOM_SEED, dim_reduction=False)\n",
    "pq_pca.train(tr_data, add=False, num_dims=num_dims, whiten=True)\n",
    "pq_pca.add(search_data, compute_distortions=False)\n",
    "results_pq_whiten_pca = compute_recall(pq_pca, R_medium, queries, exact_ranks, correct=False, sym=False)\n",
    "\n",
    "del pq_pca\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R_medium, results_pq_med[\"nearest_asym_recall_mean\"], '--s', label=f\"No Red. ({np.mean(results_pq_med['nearest_asym_recall_mean']):.2f})\", color=\"black\")\n",
    "axs[0].plot(R_medium, results_pq_whiten_pca[\"nearest_asym_recall_mean\"], '-P', label=f\"Subspace Red., {num_dims} dims, whitening ({np.mean(results_pq_whiten_pca['nearest_asym_recall_mean']):.2f})\", color=tab20(3))\n",
    "axs[0].plot(R_medium, results_pq_whiten_red_pca[\"nearest_asym_recall_mean\"], '-X', label=f\"Centroids Red., {num_dims} dims, whitening ({np.mean(results_pq_whiten_red_pca['nearest_asym_recall_mean']):.2f})\", color=tab20(2))\n",
    "\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title='Method (Mean nearest recall@R)')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(R_medium, results_pq_med[\"asym_recall_mean\"], '--s', label=f\"No Red. ({np.mean(results_pq_med['asym_recall_mean']):.2f})\", color=\"black\")\n",
    "axs[1].plot(R_medium, results_pq_whiten_pca[\"asym_recall_mean\"], '-P', label=f\"Subspace Red., {num_dims} dims, whitening ({np.mean(results_pq_whiten_pca['asym_recall_mean']):.2f})\", color=tab20(3))\n",
    "axs[1].plot(R_medium, results_pq_whiten_red_pca[\"asym_recall_mean\"], '-X', label=f\"Centroids Red., {num_dims} dims, whitening ({np.mean(results_pq_whiten_red_pca['asym_recall_mean']):.2f})\", color=tab20(2))\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].legend(title='Method (Mean recall@R)')\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle(f\"ADC, m={M}, k*={K}\\n({tr_data.shape[1]//M} dims per supspace)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to provide good quantization properties when choosing a constant value of `K`, each subvector should have, on average, a comparable inertia (sum of squared distances of samples to their closest cluster center). One way to ensure this property is to multiply the vector by a random orthogonal matrix prior to quantization. However, for most vector types, such as with SIFT descriptors, this is not required and not recommended, as consecutive components are often correlated by construction and are better quantized together with the same subquantizer. \n",
    "\n",
    "To perform such transformation, we need to call the constructor with `orth_transf=True`.\n",
    "\n",
    "Evaluating the impact of orthogonalization on the search performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_orth = PQ(M=M, K=K, seed=RANDOM_SEED, orth_transf=True)\n",
    "pq_orth.train(tr_data, add=True)\n",
    "pq_orth.add(search_data, compute_distortions=False)\n",
    "results_pq_orth = compute_recall(pq_orth, R_medium, queries, exact_ranks, correct=False, sym=False)\n",
    "results_pq_orth[\"search_NMSE\"] = NMSE(search_data, pq_orth.decompress(pq_orth.pqcode))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R_medium, results_pq_med[\"nearest_asym_recall_mean\"], '--s', label=f\"No transformation ({np.mean(results_pq_med['nearest_asym_recall_mean']):.3f})\", color='black')\n",
    "axs[0].plot(R_medium, results_pq_orth[\"nearest_asym_recall_mean\"], '-o', label=f\"Orthogonal transformation ({np.mean(results_pq_orth['nearest_asym_recall_mean']):.3f})\")\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title='Method (Mean nearest recall@R)')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(R_medium, results_pq_med[\"asym_recall_mean\"], '--s', label=f\"No transformation ({np.mean(results_pq_med['asym_recall_mean']):.3f})\", color='black')\n",
    "axs[1].plot(R_medium, results_pq_orth[\"asym_recall_mean\"], '-o', label=f\"Orthogonal transformation ({np.mean(results_pq_orth['asym_recall_mean']):.3f})\")\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].legend(title='Method (Mean recall@R)')\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle(f\"ADC, m={M}, k*={K}\\n({tr_data.shape[1] // M} dims per subspace)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying NMSE on search data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    [results_pq, results_pq_orth,],\n",
    "    index=[\"No transformation\", \"Orthogonal transformation\"]\n",
    ")[[\"search_NMSE\"]].style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing KMeans inertia in each subspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1)\n",
    "\n",
    "axs.bar(range(1, results_pq[\"inertia\"].shape[0]+1), results_pq[\"inertia\"],\n",
    "        zorder=3, edgecolor='black', linewidth=1.1, alpha=0.55, label=f\"No transformation ({np.std(results_pq['inertia']):.3f})\")\n",
    "axs.bar(range(1, results_pq_orth[\"inertia\"].shape[0]+1), results_pq_orth[\"inertia\"],\n",
    "        zorder=3, edgecolor='black', linewidth=1.1, alpha=0.55, label=f\"Orthogonal transformation ({np.std(results_pq_orth['inertia']):.3f})\")\n",
    "plt.axhline(np.mean(results_pq[\"inertia\"]), linestyle='--', label=\"Mean inertia with no transformation\", color=tab10(0))\n",
    "plt.axhline(np.mean(results_pq_orth[\"inertia\"]), linestyle='--', label=\"Mean inertia with orthogonal transformation\", color=tab10(1))\n",
    "axs.set_xlabel('Subspace')\n",
    "axs.set_ylabel('Inertia')\n",
    "axs.grid(zorder=0)\n",
    "axs.legend(loc='lower center', title=\"Method (std inertia)\")\n",
    "axs.set_title(f\"m={M}, k*={K}\\n({tr_data.shape[1] // M} dims per subspace)\");\n",
    "\n",
    "del pq_orth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore the following criteria for grouping components into subspaces:\n",
    "1. Agglomerative Hierarchical clustering of features based on their euclidean distance with different linkage criteria.\n",
    "2. Agglomerative Hierarchical clustering of features based on inverse correlation (absolute value of pearson correlation) with different linkage criteria.\n",
    "3. KMeans clustering of features.\n",
    "4. Constrained KMeans clustering of features, where the clusters have equal size.\n",
    "5. Spectral Biclustering of the data (clusters vectors and features simultaneously).\n",
    "\n",
    "Criteria 1., 3., 4. and 5. group similar features together (according to a distance metric), while criterion 2. groups features that are not correlated together.\n",
    "\n",
    "Note that, except for criterion 4., the resulting partitions are not necessarily equal in size.\n",
    "\n",
    "Performing agglomerative hierarchical clustering of features with different linkage criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_info = {}\n",
    "clusters_info['method'] = []\n",
    "clusters_info['labels'] = []\n",
    "clusters_info['clusters_sizes'] = []\n",
    "clusters_info['silhouette'] = []\n",
    "\n",
    "dist_mat = pdist(tr_data.T, metric='euclidean')\n",
    "\n",
    "algorithms = [\"single\", \"complete\", \"average\", \"ward\"]\n",
    "for algorithm in algorithms:\n",
    "    linkage_res = linkage(dist_mat, method=algorithm, metric='euclidean', optimal_ordering=False)\n",
    "    clusters = fcluster(linkage_res, t=M, criterion='maxclust')\n",
    "    counts = np.bincount(clusters)\n",
    "    silhouette = silhouette_score(squareform(dist_mat), clusters, metric='precomputed')\n",
    "    clusters_info['method'].append(algorithm)\n",
    "    clusters_info['labels'].append(clusters)\n",
    "    clusters_info['clusters_sizes'].append(counts[counts!=0])\n",
    "    clusters_info['silhouette'].append(silhouette)\n",
    "\n",
    "clusters_info_df = pd.DataFrame(clusters_info)\n",
    "clusters_info_df[[\n",
    "        'method', 'clusters_sizes', 'silhouette'\n",
    "    ]].style.background_gradient(cmap='Blues', subset=['silhouette'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"siftsmall\":\n",
    "    hier_dist_labels = clusters_info_df[clusters_info_df['method']=='complete']['labels'].values[0]\n",
    "elif DATASET == \"glove\":\n",
    "    hier_dist_labels = clusters_info_df[clusters_info_df['method']=='ward']['labels'].values[0]\n",
    "else:\n",
    "    hier_dist_labels = clusters_info_df[clusters_info_df['method']=='ward']['labels'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping features by running Constrained KMeans, KMeans and Spectral Biclustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_size = tr_data.shape[1] // M\n",
    "km_const = KMeansConstrained(n_clusters=M, size_min=clusters_size, size_max=clusters_size, random_state=RANDOM_SEED).fit(tr_data.T)\n",
    "km_const_labels = km_const.labels_\n",
    "\n",
    "km = KMeans(n_clusters=M, n_init=1, random_state=RANDOM_SEED).fit(tr_data.T)\n",
    "km_labels = km.labels_\n",
    "\n",
    "sbc = SpectralBiclustering(n_clusters=(K, M), n_init=1, random_state=RANDOM_SEED).fit(tr_data)\n",
    "sbc_labels = sbc.column_labels_\n",
    "\n",
    "partitioning_labels = {\n",
    "    \"Hierarchical\": hier_dist_labels,\n",
    "    \"K-Means Equal\": km_const_labels,\n",
    "    \"K-Means\": km_labels,\n",
    "    \"Spectral Bi-Clustering\": sbc_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the impact of different component grouping strategies on the search performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning_results = {}\n",
    "for approach, labels in partitioning_labels.items():\n",
    "    print(f\"Running PQ with '{approach}' partitioning approach\")\n",
    "    pq_opt_part = PQ(M=len(np.unique(labels)), K=K, seed=RANDOM_SEED)\n",
    "    pq_opt_part.train(tr_data, add=True, features_labels=labels)\n",
    "    pq_opt_part.add(search_data, compute_distortions=False)\n",
    "    results_pq_opt_part = compute_recall(pq_opt_part, R_medium, queries, exact_ranks, correct=False, sym=False)\n",
    "    results_pq_opt_part[\"search_NMSE\"] = NMSE(search_data, pq_opt_part.decompress(pq_opt_part.pqcode))\n",
    "    results_pq_opt_part[\"cluster_sizes\"] = pq_opt_part.features_cluster_sizes\n",
    "    partitioning_results[approach] = results_pq_opt_part\n",
    "del pq_opt_part\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R_medium, results_pq_med[\"nearest_asym_recall_mean\"], '--s', label=f\"Natural ({np.mean(results_pq_med['nearest_asym_recall_mean']):.2f})\", color=\"black\")\n",
    "for (approach, results), marker in zip(partitioning_results.items(), ['-o', '-v', '-D', '-P']):\n",
    "    axs[0].plot(R_medium, results[\"nearest_asym_recall_mean\"], marker, label=f\"{approach} ({np.mean(results['nearest_asym_recall_mean']):.2f})\")\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title='Method (Mean nearest recall@R)')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(R_medium, results_pq_med[\"asym_recall_mean\"], '--s', label=f\"Natural ({np.mean(results_pq_med['asym_recall_mean']):.2f})\", color=\"black\")\n",
    "for (approach, results), marker in zip(partitioning_results.items(), ['-o', '-v', '-D', '-P']):\n",
    "    axs[1].plot(R_medium, results[\"asym_recall_mean\"], marker, label=f\"{approach} ({np.mean(results['asym_recall_mean']):.2f})\")\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].legend(title='Method (Mean recall@R)')\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle(f\"ADC, m={M}, k*={K}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying NMSE on search data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning_results[\"Natual partitioning\"] = results_pq\n",
    "results_pq[\"cluster_sizes\"] = clusters_size\n",
    "pd.DataFrame(partitioning_results.values(), index=partitioning_results.keys()\n",
    ")[[\"search_NMSE\"]].style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the resulting feature clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = []\n",
    "labels_names = []\n",
    "for approach, labels in partitioning_labels.items():\n",
    "    if approach != \"Hier. Inv. Corr.\":\n",
    "        labels_list.append(labels.tolist())\n",
    "        labels_names.append(approach)\n",
    "sankey_plot(labels_list, labels_names, title=\"Column clusters comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing KMeans inertia and the number of features in each subspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(6*5, 4), sharey=True)\n",
    "\n",
    "equal_partition_line = int(tr_data.shape[1] / M)\n",
    "\n",
    "for i, (approach, results) in enumerate(partitioning_results.items()):\n",
    "    xticks_range = range(1, len(results[\"inertia\"]) + 1)\n",
    "\n",
    "    axs[i].bar(xticks_range, results[\"inertia\"], zorder=3, label=approach)\n",
    "    axs[i].set_xlabel(\"Subspace\")\n",
    "    axs[i].set_ylabel(\"Inertia\")\n",
    "    axs[i].set_title(approach+f\"\\n(Mean Inertia = {np.mean(results['inertia']):.2f})\")\n",
    "    if DATASET != \"gist\":\n",
    "        axs[i].set_xticks(xticks_range)\n",
    "    axs[i].grid(zorder=0)\n",
    "\n",
    "    ax_twin = axs[i].twinx()\n",
    "    ax_twin.bar(\n",
    "        xticks_range, results[\"cluster_sizes\"],\n",
    "        zorder=3, facecolor=\"none\", edgecolor=\"black\", label=\"Features per subspace\"\n",
    "    )\n",
    "    ax_twin.axhline(\n",
    "        equal_partition_line, color=\"black\", linestyle=\"--\",\n",
    "        label=\"Features per subspace\\nby equal partition\"\n",
    "    )\n",
    "    ax_twin.set_ylabel(\"Features per cluster\")\n",
    "    if i==4:\n",
    "        ax_twin.legend(loc=\"upper right\", bbox_to_anchor=(1.7, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the n-th nearest neighbor distances of the vectors in the training data for different values of n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"siftsmall\":\n",
    "    pq_weighted = PQ(M=M, K=K, seed=RANDOM_SEED)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n",
    "    pq_weighted.plot_neighbor_distances(data=tr_data, neighbor=3, ax=axs[0])\n",
    "    pq_weighted.plot_neighbor_distances(data=tr_data, neighbor=5, ax=axs[1])\n",
    "    pq_weighted.plot_neighbor_distances(data=tr_data, neighbor=7, ax=axs[2])\n",
    "    pq_weighted.plot_neighbor_distances(data=tr_data, neighbor=9, ax=axs[3])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assess the effect of weighting samples during KMeans training by considering their distance to the nth-nearest neighbor exploring various values of n and weighting strategies. The nth-nearest neighbor distance serves as a measure of local data density. By assigning weights inversely proportional to this distance, we prioritize points that are more representative of the dataset. On the other hand, assigning weights directly proportional to the nth-nearest neighbor distance prioritizes points that are more spread out, preventing overfitting in regions with a high density of data.\n",
    "\n",
    "This is implemented by calling the `train` method with `weight_samples=True` and specifying the number of neighbors (`neighbor` parameter), the weighting method (`weight_method` parameter) and whether to use inverse weights (`inverse_weights` parameter).\n",
    "\n",
    "The following weighting methods are considered:\n",
    "- with `inverse_weights=True`:\n",
    "    - when `weight_method='normal'`, weights are 1 - distances normalized to [0, 1]\n",
    "    - when `weight_method='reciprocal'`, weights are 1 / distances, subsequently normalized to [0, 1] \n",
    "- with `inverse_weights=False`:\n",
    "    - weights are distances normalized to [0, 1], independently of the weighting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_PQ_recall(\n",
    "    pq, weight_samples, neighbor, inverse_weights, weight_method, R, tr_data,\n",
    "    search_data, queries, exact_ranks):\n",
    "    pq.train(\n",
    "        tr_data, add=False, weight_samples=weight_samples,\n",
    "        neighbor=neighbor, inverse_weights=inverse_weights,\n",
    "        weight_method=weight_method)\n",
    "    pq.add(search_data, compute_distortions=False)\n",
    "    results = compute_recall(pq, R, queries, exact_ranks, correct=False, sym=False)\n",
    "    decompressed_search = pq.decompress(pq.pqcode)\n",
    "    error_search = NMSE(search_data, decompressed_search)\n",
    "    results[\"search_NMSE\"] = error_search\n",
    "    return results\n",
    "\n",
    "if DATASET == \"siftsmall\":\n",
    "    neighbors = [3, 5, 7, 9]\n",
    "    weight_methods = [\"normal\", \"reciprocal\"]\n",
    "    inverse_weights_opts = [True, False]\n",
    "\n",
    "    search_NMSE_weighted = []\n",
    "    nearest_recall_weithed = []\n",
    "    recall_weighted = []\n",
    "    for neighbor in neighbors:\n",
    "        for weight_method in weight_methods:\n",
    "            for inverse_weights in inverse_weights_opts:\n",
    "                if (weight_method == \"reciprocal\" and inverse_weights == False):\n",
    "                    continue\n",
    "                results = compute_weighted_PQ_recall(\n",
    "                    pq_weighted, True, neighbor, inverse_weights, weight_method,\n",
    "                    R_small, tr_data, search_data, queries, exact_ranks)\n",
    "                keys = {\"neighbor\": neighbor, \"weight_method\": weight_method, \"inverse_weights\": inverse_weights}\n",
    "                search_NMSE_weighted.append({**keys, \"search_NMSE\": results[\"search_NMSE\"]})\n",
    "                nearest_recall_weithed.append({**keys, \"nearest_asym_recall_mean\": results[\"nearest_asym_recall_mean\"]})\n",
    "                recall_weighted.append({**keys, \"asym_recall_mean\": results[\"asym_recall_mean\"]})\n",
    "    del pq_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"siftsmall\":\n",
    "    nearest_recall_weithed_df = pd.DataFrame(nearest_recall_weithed)\n",
    "    recall_weithed_df = pd.DataFrame(recall_weighted)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(22, 5), sharey=True)\n",
    "    markers = ['o', 'v', 'D', 'P']\n",
    "    i=0\n",
    "    for inverse_weights in inverse_weights_opts:\n",
    "        for weight_method in weight_methods:\n",
    "            if (weight_method == \"reciprocal\" and inverse_weights == False):\n",
    "                continue\n",
    "            axs[i].plot(R_small, results_pq_seeds[0][\"nearest_asym_recall_mean\"], label=f\"None ({np.mean(results_pq_seeds[0]['nearest_asym_recall_mean']):.2f})\", linestyle='--', marker='s', color=\"black\")\n",
    "            for j, neighbor in enumerate(neighbors):\n",
    "                recall = nearest_recall_weithed_df[\n",
    "                    (nearest_recall_weithed_df['neighbor']==neighbor) &\n",
    "                    (nearest_recall_weithed_df['weight_method']==weight_method) &\n",
    "                    (nearest_recall_weithed_df['inverse_weights']==inverse_weights)\n",
    "                ]['nearest_asym_recall_mean'].values[0]\n",
    "                axs[i].plot(R_small, recall, label=f\"{neighbor} ({np.mean(recall):.2f})\", marker=markers[j])\n",
    "            axs[i].set_xlabel('R')\n",
    "            axs[i].set_ylabel('Nearest recall@R')\n",
    "            if weight_method == \"normal\" and inverse_weights == True:\n",
    "                title = r\"$w(i)=1-\\frac{d_{n-th}(i)}{\\max_j\\:d_{n-th}(j)}$\"\n",
    "            elif weight_method == \"normal\" and inverse_weights == False:\n",
    "                title = r\"$\\frac{d_{n-th}(i)}{\\max_j\\:d_{n-th}(j)}$\"\n",
    "            else:\n",
    "                title = r\"$w(i)=\\frac{\\frac{1}{d_{n-th}(i)}}{\\max_j\\:\\frac{1}{d_{n-th}(j)}}$\"\n",
    "            axs[i].title.set_text(title)\n",
    "            axs[i].legend(title='n (Mean nearest recall@R)')\n",
    "            axs[i].grid()\n",
    "            i+=1\n",
    "    fig.suptitle(f\"ADC, m={M}, k*={K}\\n({tr_data.shape[1] // M} dims per subspace)\");\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(22, 5), sharey=True)\n",
    "    markers = ['o', 'v', 'D', 'P']\n",
    "    i=0\n",
    "    for inverse_weights in inverse_weights_opts:\n",
    "        for weight_method in weight_methods:\n",
    "            if (weight_method == \"reciprocal\" and inverse_weights == False):\n",
    "                continue\n",
    "            axs[i].plot(R_small, results_pq_seeds[0][\"asym_recall_mean\"], label=f\"None ({np.mean(results_pq_seeds[0]['asym_recall_mean']):.2f})\", linestyle='--', marker='s', color=\"black\")\n",
    "            for j, neighbor in enumerate(neighbors):\n",
    "                recall = recall_weithed_df[\n",
    "                        (nearest_recall_weithed_df['neighbor']==neighbor) &\n",
    "                        (nearest_recall_weithed_df['weight_method']==weight_method) &\n",
    "                        (nearest_recall_weithed_df['inverse_weights']==inverse_weights)\n",
    "                    ]['asym_recall_mean'].values[0]\n",
    "                axs[i].plot(R_small, recall, label=f\"{neighbor} ({np.mean(recall):.2f})\", marker=markers[j])\n",
    "            axs[i].set_xlabel('R')\n",
    "            axs[i].set_ylabel('Recall@R')\n",
    "            if weight_method == \"normal\" and inverse_weights == True:\n",
    "                title = r\"$w(i)=1-\\frac{d_{n-th}(i)}{\\max_j\\:d_{n-th}(j)}$\"\n",
    "            elif weight_method == \"normal\" and inverse_weights == False:\n",
    "                title = r\"$\\frac{d_{n-th}(i)}{\\max_j\\:d_{n-th}(j)}$\"\n",
    "            else:\n",
    "                title = r\"$w(i)=\\frac{\\frac{1}{d_{n-th}(i)}}{\\max_j\\:\\frac{1}{d_{n-th}(j)}}$\"\n",
    "            axs[i].title.set_text(title)\n",
    "            axs[i].legend(title='n (Mean recall@R)')\n",
    "            axs[i].grid()\n",
    "            i+=1\n",
    "    fig.suptitle(f\"ADC, m={M}, k*={K}\\n({tr_data.shape[1] // M} dims per subspace)\");\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying NMSE on search data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"siftsmall\":\n",
    "    pd.DataFrame(search_NMSE_weighted).style.background_gradient(subset=[\"search_NMSE\"], cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the approach introduced by Tibshirani et al. in \"Diagnosis of multiple cancer types by shrunken centroids of gene expression\" to modify the centroids identified by the KMeans algorithm. The value of each feature for each centroid is divided by the within-centroid variance of that feature. The feature values are then reduced by shrink_threshold. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features.\n",
    "\n",
    "This feature is implemented by calling the constructor and specifying the parameter `shrink_threshold`. We used the [sklearn implementation of the method](https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.NearestCentroid.html#nearestcentroid).\n",
    "\n",
    "\n",
    "Fitting PQ with different shrinkage thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebooks = {}\n",
    "\n",
    "codebooks[\"Shrink threshold = None\"] = pq.codebook\n",
    "del pq\n",
    "\n",
    "thresholds = [0.5, 0.75, 1] if DATASET == \"siftsmall\" else [0.1, 0.25, 0.5]\n",
    "shrunken_pqs = []\n",
    "shrunkes_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    pq = PQ(M=M, K=K, seed=RANDOM_SEED, shrink_threshold=threshold)\n",
    "    start_training = time.time()\n",
    "    pq.train(tr_data, add=False)\n",
    "    training_time = time.time() - start_training\n",
    "    codebooks[f\"Shrink threshold = {threshold:.2f}\"] = pq.codebook\n",
    "\n",
    "    start_adding = time.time()\n",
    "    pq.add(search_data, compute_distortions=True)\n",
    "    adding_time = time.time() - start_adding\n",
    "\n",
    "    decompressed_search = pq.decompress(pq.pqcode)\n",
    "    error_search = NMSE(search_data, decompressed_search)\n",
    "\n",
    "    compressed_tr = pq.compress(tr_data)\n",
    "    decompressed_tr = pq.decompress(compressed_tr)\n",
    "    error_tr = NMSE(tr_data, decompressed_tr)\n",
    "\n",
    "    shrunken_pqs.append(pq)\n",
    "    shrunkes_results.append({\n",
    "        \"Training time [s]\": training_time, \"Adding time [s]\": adding_time,\n",
    "        \"NMSE on training data\": error_tr, \"NMSE on search data\": error_search\n",
    "    })\n",
    "\n",
    "shrunken_pq_results_df = pd.DataFrame(\n",
    "    shrunkes_results,\n",
    "    index=[f\"Shrunken PQ, shrink_threshold={threshold:.2f}\" for threshold in thresholds]\n",
    ")\n",
    "\n",
    "pq_results_df.drop(\n",
    "    columns=[\"Compressed data shape\", \"Compressed data size [bytes]\",\n",
    "    \"Original data size [bytes]\", \"Compression factor\"],\n",
    "    inplace=True)\n",
    "\n",
    "pd.concat([pq_results_df, shrunken_pq_results_df], axis=0).style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting subspace centroids with different shrinkage thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method, codebook in codebooks.items():\n",
    "    plt.plot(codebook[0][0], '-o', label=f\"Subspace 0, Centroid 0, {method}\")\n",
    "\n",
    "for method, codebook in codebooks.items():\n",
    "    plt.plot(codebook[0][1], '-o', label=f\"Subspace 1, Centroid 0, {method}\")\n",
    "\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), shadow=True, ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the impact of the shrinkage threshold on the search performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrunken_recalls = {}\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    shrunken_recalls[threshold] = compute_recall(shrunken_pqs[i], R_medium, queries, exact_ranks, correct=False, sym=False)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R_medium, results_pq_med[\"nearest_asym_recall_mean\"], '--s', label=f\"PQ ({np.mean(results_pq_med['nearest_asym_recall_mean']):.2f})\", color=\"black\")\n",
    "for i, (threshold, m) in enumerate(zip(thresholds, ['-o', '-v', '-D', '-P'])):\n",
    "    axs[0].plot(R_medium, shrunken_recalls[threshold][\"nearest_asym_recall_mean\"], m, label=f\"Shrunken PQ, shrink_threshold={threshold} ({np.mean(shrunken_recalls[threshold]['nearest_asym_recall_mean']):.2f})\")\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title='Method (Mean nearest recall@R)')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(R_medium, results_pq_med[\"asym_recall_mean\"], '--s', label=f\"PQ ({np.mean(results_pq_med['asym_recall_mean']):.2f})\", color=\"black\")\n",
    "for i, (threshold, m) in enumerate(zip(thresholds, ['-o', '-v', '-D', '-P'])):\n",
    "    axs[1].plot(R_medium, shrunken_recalls[threshold][\"asym_recall_mean\"], m, label=f\"Shrunken PQ, shrink_threshold={threshold} ({np.mean(shrunken_recalls[threshold]['asym_recall_mean']):.2f})\")\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('R')\n",
    "axs[1].set_ylabel('Recall@R')\n",
    "axs[1].legend(title='Method (Mean recall@R)')\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle(f\"ADC, m={M}, k*={K}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Quantization with InVerted File index (IVF)\n",
    "\n",
    "Computing the average recall at various values of R for the set of queries on scaled data with different scaling approaches. Here, KMeans inertia (sum of squared distances of samples to their closest cluster center) is computed for the coarse clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivf_std = IVF(Kp=KP, M=M, K=K, seed=RANDOM_SEED)\n",
    "ivf_std.train(tr_data_std, add=False)\n",
    "ivf_std.add(search_data_std, compute_distortions=True)\n",
    "results_ivf_std = compute_recall(ivf_std, R, queries_std, exact_ranks_std, w=W)\n",
    "del ivf_std\n",
    "\n",
    "ivf_minmax = IVF(Kp=KP, M=M, K=K, seed=RANDOM_SEED)\n",
    "ivf_minmax.train(tr_data_minmax, add=False)\n",
    "ivf_minmax.add(search_data_minmax, compute_distortions=True)\n",
    "results_ivf_minmax = compute_recall(ivf_minmax, R, queries_minmax, exact_ranks_minmax, w=W)\n",
    "del ivf_minmax\n",
    "\n",
    "def compute_overall_inertia(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    return np.sum(np.square(data - mean))\n",
    "\n",
    "overall_inertia = compute_overall_inertia(tr_data)\n",
    "overall_inertia_std = compute_overall_inertia(tr_data_std)\n",
    "overall_inertia_minmax = compute_overall_inertia(tr_data_minmax)\n",
    "\n",
    "scaling_methods_ivf = [\n",
    "    (\"Raw data\", results_ivf, overall_inertia),\n",
    "    (\"Standard scaling\", results_ivf_std, overall_inertia_std),\n",
    "    (\"Min-max scaling\", results_ivf_minmax, overall_inertia_minmax)\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "for ax, (title, data, overall_inertia) in zip(axs, scaling_methods_ivf):\n",
    "    ax.plot(R, data[\"nearest_asym_recall_mean\"], '-s', label=f\"IVFADC ({np.nanmean(data['nearest_asym_recall_mean']):.2f})\", color=tab20(0))\n",
    "    ax.plot(R, data[\"nearest_asym_corr_recall_mean\"], '-D', label=f\"Corrected IVFADC ({np.nanmean(data['nearest_asym_corr_recall_mean']):.2f})\", color=tab20(1))\n",
    "    ax.plot(R, data[\"nearest_sym_recall_mean\"], '-o', label=f\"IVFSDC ({np.nanmean(data['nearest_sym_recall_mean']):.2f})\", color=tab20(6))\n",
    "    ax.plot(R, data[\"nearest_sym_corr_recall_mean\"], '-p', label=f\"Corrected IVFSDC ({np.nanmean(data['nearest_sym_corr_recall_mean']):.2f})\", color=tab20(7))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('R')\n",
    "    ax.set_ylabel('Nearest recall@R')\n",
    "    ax.set_title(f\"{title} (relative inertia = {1-(data['inertia'] / overall_inertia):.2f})\")\n",
    "    ax.legend(title=\"Method (Mean nearest recall@R)\")\n",
    "    ax.grid()\n",
    "fig.suptitle(f\"m={M}, k*={K}, k'={KP}\\n({tr_data.shape[1] // M} dims per subspace)\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the search performance and training time when using KMeans, MiniBatch KMeans (with default parameters, i.e. `batch_size=1024`) and Bisecting KMeans for the coarse clustering with different parameters. This is implemented by calling the IVF constructor and specifying the parameter `coarse_clust_alg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IVF_recall(\n",
    "        M, K, KP, W, coarse_clust_alg, tr_data, search_data, queries,\n",
    "        exact_ranks, seed=RANDOM_SEED):\n",
    "    \n",
    "    ivf = IVF(Kp=KP, M=M, K=K, seed=seed, coarse_clust_alg=coarse_clust_alg)\n",
    "    start_training_ivf = time.time()\n",
    "    ivf.train(tr_data, add=False)\n",
    "    training_ivf = time.time() - start_training_ivf\n",
    "    ivf.add(search_data, compute_distortions=False)\n",
    "    results_ivf = compute_recall(ivf, R, queries, exact_ranks, w=W, correct=False, sym=False)\n",
    "    return results_ivf, training_ivf\n",
    "\n",
    "results_ivf_mini, training_ivf_mini = compute_IVF_recall(\n",
    "    M, K, KP, W, \"mkm\", tr_data, search_data, queries, exact_ranks, seed=RANDOM_SEED)\n",
    "results_ivf_bis, training_ivf_bis = compute_IVF_recall(\n",
    "    M, K, KP, W, \"bkm\", tr_data, search_data, queries, exact_ranks, seed=RANDOM_SEED)\n",
    "results_ivf_double, training_ivf_double = compute_IVF_recall(\n",
    "    M, K, KP*2, W*2, \"km\", tr_data, search_data, queries, exact_ranks, seed=RANDOM_SEED)\n",
    "results_ivf_mini_double, training_ivf_mini_double = compute_IVF_recall(\n",
    "    M, K, KP*2, W*2, \"mkm\", tr_data, search_data, queries, exact_ranks, seed=RANDOM_SEED)\n",
    "results_ivf_bis_double, training_ivf_bis_double = compute_IVF_recall(\n",
    "    M, K, KP*2, W*2, \"bkm\", tr_data, search_data, queries, exact_ranks, seed=RANDOM_SEED)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(R, results_ivf[\"nearest_asym_recall_mean\"], \"--s\", label=f\"k'={KP}, w={W}, KMeans ({np.nanmean(results_ivf['nearest_asym_recall_mean']):.2f})\", color=\"black\") \n",
    "axs[0].plot(R, results_ivf_mini[\"nearest_asym_recall_mean\"], \"-o\", label=f\"k'={KP}, w={W}, Mini-batch KMeans ({np.nanmean(results_ivf_mini['nearest_asym_recall_mean']):.2f})\", color=\"orange\")\n",
    "axs[0].plot(R, results_ivf_bis[\"nearest_asym_recall_mean\"], \"-x\", label=f\"k'={KP}, w={W}, Bisecting KMeans ({np.nanmean(results_ivf_bis['nearest_asym_recall_mean']):.2f})\", color=\"red\")\n",
    "axs[0].plot(R, results_ivf_double[\"nearest_asym_recall_mean\"], \"-.d\", label=f\"k'={KP*2}, w={W*2}, KMeans ({np.nanmean(results_ivf_double['nearest_asym_recall_mean']):.2f})\", color=\"gray\")\n",
    "axs[0].plot(R, results_ivf_mini_double[\"nearest_asym_recall_mean\"], \"-v\", label=f\"k'={KP*2}, w={W*2}, Mini-batch KMeans ({np.nanmean(results_ivf_mini_double['nearest_asym_recall_mean']):.2f})\", color=\"cornflowerblue\")\n",
    "axs[0].plot(R, results_ivf_bis_double[\"nearest_asym_recall_mean\"], \"-P\", label=f\"k'={KP*2}, w={W*2}, Bisecting KMeans ({np.nanmean(results_ivf_bis_double['nearest_asym_recall_mean']):.2f})\", color=\"blue\")\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('R')\n",
    "axs[0].set_ylabel('Nearest recall@R')\n",
    "axs[0].legend(title=\"Method (Mean nearest recall@R)\")\n",
    "axs[0].grid()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"k'\": [KP, KP*2],\n",
    "    \"Training time Bisecting KMeans\": [training_ivf_bis, training_ivf_bis_double],\n",
    "    \"Training time KMeans\": [training_ivf, training_ivf_double],\n",
    "    \"Training time Mini-batch KMeans\": [training_ivf_mini, training_ivf_mini_double]\n",
    "    })\n",
    "df.set_index(\"k'\", inplace=True)\n",
    "ax = df.plot.bar(rot=0, ax=axs[1])\n",
    "colors = [\"black\", \"orange\", \"red\", \"gray\", \"cornflowerblue\", \"blue\"]\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    if i%2==0:\n",
    "        bar.set_color(colors[:3][i//2])\n",
    "    else:\n",
    "        bar.set_color(colors[3:][i//2])\n",
    "legend_patches = []\n",
    "for i in range(6):\n",
    "    w = W if i < 3 else W*2\n",
    "    if i%3==0:\n",
    "        coarse_clust_alg = \"KMeans\" \n",
    "    elif i%3==1:\n",
    "        coarse_clust_alg = \"Mini-batch KMeans\"\n",
    "    else:\n",
    "        coarse_clust_alg = \"Bisecting KMeans\"\n",
    "    label = f\"w={w}, {coarse_clust_alg}\"\n",
    "    legend_patches.append(mpatches.Patch(color=colors[i], label=label))\n",
    "ax.legend(handles=legend_patches, title=\"Method\", loc=\"lower right\")\n",
    "ax.set_ylabel(\"Training time [s]\");\n",
    "\n",
    "fig.suptitle(f\"IVFADC, m={M}, k*={K}\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
